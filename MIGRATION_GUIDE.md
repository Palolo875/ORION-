# Guide de Migration - Optimisations Mod√®les ORION

## üìã Vue d'Ensemble

Ce guide explique comment migrer le code existant pour utiliser les nouvelles optimisations.

---

## üîÑ √âtape 1: Migrer les Workers vers le Service d'Embedding Partag√©

### Avant (Workers Ind√©pendants)

Chaque worker charge son propre mod√®le d'embedding:

```typescript
// memory.worker.ts, migration.worker.ts, geniusHour.worker.ts
import { pipeline, env } from '@xenova/transformers';

class EmbeddingPipeline {
  static instance: PipelineInstance | null = null;
  static async getInstance(): Promise<PipelineInstance> {
    if (this.instance === null) {
      this.instance = await pipeline('feature-extraction', MEMORY_CONFIG.EMBEDDING_MODEL);
    }
    return this.instance;
  }
}

async function createEmbedding(text: string): Promise<number[]> {
  const extractor = await EmbeddingPipeline.getInstance();
  const result = await extractor(text, {});
  return Array.from(result as ArrayLike<number>);
}
```

### Apr√®s (Service Partag√©)

Tous les workers utilisent le service partag√©:

```typescript
// Dans chaque worker qui a besoin d'embeddings
import { getSharedEmbeddingClient } from '../utils/sharedEmbedding';

// Cr√©er le client (une seule fois au d√©but du worker)
const embeddingClient = getSharedEmbeddingClient();

// Utiliser pour cr√©er des embeddings
async function createEmbedding(text: string): Promise<number[]> {
  return await embeddingClient.createEmbedding(text);
}
```

### Modifications N√©cessaires

#### memory.worker.ts

```typescript
// Ligne ~15: REMPLACER les imports
import { getSharedEmbeddingClient } from '../utils/sharedEmbedding';

// Lignes ~24-40: SUPPRIMER la classe EmbeddingPipeline locale

// Ligne ~182: REMPLACER createSemanticEmbedding
async function createSemanticEmbedding(text: string): Promise<number[]> {
  const client = getSharedEmbeddingClient();
  return await client.createEmbedding(text);
}
```

#### migration.worker.ts

```typescript
// Ligne ~13: REMPLACER les imports
import { getSharedEmbeddingClient } from '../utils/sharedEmbedding';

// Lignes ~27-43: SUPPRIMER la classe EmbeddingPipeline locale

// Ligne ~46: REMPLACER createSemanticEmbedding
async function createSemanticEmbedding(text: string): Promise<number[]> {
  const client = getSharedEmbeddingClient();
  return await client.createEmbedding(text);
}
```

#### geniusHour.worker.ts

```typescript
// Ligne ~15: REMPLACER les imports
import { getSharedEmbeddingClient } from '../utils/sharedEmbedding';

// Lignes ~68-84: SUPPRIMER la classe EmbeddingPipeline locale

// Ligne ~86: REMPLACER createEmbedding
async function createEmbedding(text: string): Promise<number[]> {
  const client = getSharedEmbeddingClient();
  return await client.createEmbedding(text);
}
```

---

## üéØ √âtape 2: Int√©grer le Model Loader Optimis√©

### Dans le LLM Worker

```typescript
// src/workers/llm.worker.ts

import { getModelLoader } from '../utils/modelLoader';

async function loadModel(modelKey: string) {
  const loader = getModelLoader();
  
  // Charger avec progression
  await loader.loadModel(modelKey, (progress) => {
    self.postMessage({
      type: 'load_progress',
      payload: { progress, modelKey }
    });
  });
  
  // Marquer comme utilis√©
  loader.markModelUsed(modelKey);
}
```

### Dans les Composants React

```typescript
// src/components/ModelSelector.tsx

import { getModelLoader } from '@/utils/modelLoader';
import { useState } from 'react';

function ModelSelector() {
  const [loading, setLoading] = useState(false);
  const [progress, setProgress] = useState(0);
  
  const handleModelSelect = async (modelKey: string) => {
    setLoading(true);
    setProgress(0);
    
    try {
      const loader = getModelLoader();
      await loader.loadModel(modelKey, (p) => setProgress(p));
      
      // Notifier le succ√®s
      toast.success('Mod√®le charg√© avec succ√®s!');
    } catch (error) {
      toast.error('Erreur lors du chargement');
    } finally {
      setLoading(false);
    }
  };
  
  // ... reste du composant
}
```

---

## üìä √âtape 3: Ajouter le Monitoring M√©moire

### Dans le Layout Principal

```typescript
// src/pages/Index.tsx ou Layout.tsx

import { MemoryMonitorWidget, MemoryStatusIndicator } from '@/components/MemoryMonitorWidget';

function Layout() {
  return (
    <div>
      {/* Version compl√®te (debug/dev) */}
      {process.env.NODE_ENV === 'development' && (
        <div className="fixed bottom-4 right-4 w-80">
          <MemoryMonitorWidget showDetails />
        </div>
      )}
      
      {/* Version compacte (production) */}
      <header>
        <div className="flex items-center gap-2">
          <span>ORION</span>
          <MemoryStatusIndicator />
        </div>
      </header>
      
      {/* Contenu principal */}
      <main>
        {children}
      </main>
    </div>
  );
}
```

### Avec un Hook Personnalis√©

```typescript
// Dans n'importe quel composant

import { useMemoryMonitoring, useMemoryAlerts } from '@/hooks/useMemoryMonitoring';

function MyComponent() {
  const { pressure, isHealthy, isCritical } = useMemoryMonitoring();
  const { alerts, hasAlerts } = useMemoryAlerts();
  
  // D√©sactiver certaines fonctionnalit√©s si critique
  useEffect(() => {
    if (isCritical) {
      // D√©charger les mod√®les non essentiels
      console.warn('M√©moire critique - mode √©conomie activ√©');
    }
  }, [isCritical]);
  
  return (
    <div>
      {hasAlerts && (
        <div className="alerts">
          {alerts.map((alert, i) => (
            <Alert key={i}>{alert}</Alert>
          ))}
        </div>
      )}
    </div>
  );
}
```

---

## ‚öôÔ∏è √âtape 4: Configuration de la Quantization

### D√©tection Automatique

Le syst√®me d√©tecte automatiquement les capacit√©s de l'appareil:

```typescript
// Automatique - pas besoin de configuration
import { getOptimizationStrategy } from '@/config/modelOptimization';

// La strat√©gie est appliqu√©e automatiquement dans modelLoader
const loader = getModelLoader();
```

### Configuration Manuelle (Optionnel)

Pour forcer un niveau de quantization:

```typescript
import { applyQuantization, QUANTIZATION_LEVELS } from '@/config/modelOptimization';

// Forcer q4 pour Mistral
const modelId = 'Mistral-7B-Instruct-v0.2';
const quantizedId = applyQuantization(modelId, 'q4');
// R√©sultat: 'Mistral-7B-Instruct-v0.2-q4f16_1-MLC'

// Charger avec quantization forc√©e
await loader.loadModel(quantizedId);
```

---

## üß™ √âtape 5: Tests et Validation

### Test de l'Embedding Partag√©

```typescript
// test/shared-embedding.test.ts

import { getSharedEmbeddingClient } from '@/utils/sharedEmbedding';

describe('SharedEmbeddingClient', () => {
  it('should create embeddings', async () => {
    const client = getSharedEmbeddingClient();
    const embedding = await client.createEmbedding('test');
    
    expect(embedding).toBeDefined();
    expect(embedding.length).toBeGreaterThan(0);
  });
  
  it('should use cache for same text', async () => {
    const client = getSharedEmbeddingClient();
    
    const start1 = performance.now();
    await client.createEmbedding('test repeated');
    const time1 = performance.now() - start1;
    
    const start2 = performance.now();
    await client.createEmbedding('test repeated');
    const time2 = performance.now() - start2;
    
    // Le 2√®me appel devrait √™tre beaucoup plus rapide (cache)
    expect(time2).toBeLessThan(time1 * 0.1);
  });
});
```

### Test du Model Loader

```typescript
// test/model-loader.test.ts

import { getModelLoader } from '@/utils/modelLoader';
import { MODELS } from '@/config/models';

describe('ModelLoader', () => {
  it('should load model with progress', async () => {
    const loader = getModelLoader();
    const progressValues: number[] = [];
    
    await loader.loadModel('demo', (progress) => {
      progressValues.push(progress);
    });
    
    expect(progressValues.length).toBeGreaterThan(0);
    expect(Math.max(...progressValues)).toBe(100);
  });
  
  it('should respect max concurrent models', async () => {
    const loader = getModelLoader();
    const stats = loader.getStats();
    
    expect(stats.loadedModels).toBeLessThanOrEqual(stats.maxConcurrentModels);
  });
});
```

### Test du Memory Monitor

```typescript
// test/memory-monitor.test.ts

import { getMemoryMonitor } from '@/utils/performance/memoryMonitor';

describe('MemoryMonitor', () => {
  it('should take snapshots', async () => {
    const monitor = getMemoryMonitor();
    const snapshot = await monitor.takeSnapshot();
    
    expect(snapshot).toBeDefined();
    expect(snapshot.pressure).toMatch(/low|medium|high|critical/);
    expect(snapshot.recommendations).toBeInstanceOf(Array);
  });
  
  it('should track history', async () => {
    const monitor = getMemoryMonitor();
    
    await monitor.takeSnapshot();
    await monitor.takeSnapshot();
    
    const snapshots = monitor.getSnapshots();
    expect(snapshots.length).toBeGreaterThan(1);
  });
});
```

---

## üöÄ √âtape 6: D√©ploiement

### Checklist Pr√©-D√©ploiement

- [ ] Migrer tous les workers vers le service d'embedding partag√©
- [ ] Tester le chargement de chaque mod√®le avec le nouveau loader
- [ ] V√©rifier que le monitoring m√©moire fonctionne
- [ ] Tester sur appareil low-end (2GB RAM)
- [ ] Tester sur appareil high-end (8GB+ RAM)
- [ ] V√©rifier les logs pour d√©tecter les erreurs
- [ ] Mesurer les m√©triques avant/apr√®s

### Configuration de Production

```typescript
// vite.config.ts

export default defineConfig({
  // ... autres configs
  define: {
    'process.env.ENABLE_MEMORY_MONITOR': JSON.stringify(
      process.env.NODE_ENV === 'development'
    ),
  },
});
```

### Variables d'Environnement

```bash
# .env.production

# Activer/d√©sactiver le monitoring
VITE_ENABLE_MEMORY_MONITOR=false

# Niveau de quantization par d√©faut
VITE_DEFAULT_QUANTIZATION=q4

# Nombre max de mod√®les charg√©s
VITE_MAX_CONCURRENT_MODELS=2
```

---

## üìà M√©triques √† Surveiller

### Avant la Migration

1. **Ouvrir DevTools Console**
2. **Noter les m√©triques:**
   ```javascript
   // Dans la console
   performance.memory.usedJSHeapSize / (1024 * 1024) // MB
   ```

### Apr√®s la Migration

1. **Comparer les m√©triques:**
   - Utilisation m√©moire: devrait √™tre r√©duite de ~60%
   - Temps de chargement: devrait √™tre r√©duit de ~40%
   - Nombre de mod√®les charg√©s: devrait respecter les limites

2. **Utiliser le widget de monitoring:**
   ```typescript
   const monitor = getMemoryMonitor();
   const stats = monitor.getStats();
   console.log('Stats:', stats);
   ```

---

## ‚ö†Ô∏è Probl√®mes Courants et Solutions

### Probl√®me 1: "Worker failed to initialize"

**Cause:** Le worker partag√© ne se charge pas correctement

**Solution:**
```typescript
// V√©rifier que le worker est bien configur√© dans vite.config.ts
worker: {
  format: 'es',
}
```

### Probl√®me 2: "Memory quota exceeded"

**Cause:** Trop de mod√®les charg√©s simultan√©ment

**Solution:**
```typescript
// Forcer le d√©chargement des mod√®les
const loader = getModelLoader();
await loader.unloadModel('model-id');
```

### Probl√®me 3: "Embedding creation timeout"

**Cause:** Le service partag√© est surcharg√©

**Solution:**
```typescript
// Augmenter le timeout dans sharedEmbedding.ts
const timeout = window.setTimeout(() => {
  // ...
}, 60000); // 60 secondes au lieu de 30
```

---

## üéì Ressources Suppl√©mentaires

- [Documentation Quantization](./docs/quantization.md)
- [Guide Performance](./docs/performance.md)
- [Architecture Workers](./docs/workers-architecture.md)

---

## ‚úÖ Validation Finale

Une fois la migration termin√©e, v√©rifier:

1. ‚úÖ Tous les workers utilisent le service d'embedding partag√©
2. ‚úÖ Les 3 nouveaux mod√®les se chargent avec quantization
3. ‚úÖ Le monitoring m√©moire affiche des donn√©es
4. ‚úÖ Les tests passent
5. ‚úÖ L'utilisation m√©moire a diminu√© de ~60%
6. ‚úÖ Aucune r√©gression de qualit√©

**Commande de validation:**
```bash
npm run test
npm run build
npm run lint
```

---

**Date:** 2025-10-21  
**Version:** 1.0.0
