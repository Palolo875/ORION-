{
  "$schema": "./models.schema.json",
  "version": "1.0.0",
  "last_updated": "2025-10-24",
  "models": {
    "demo-agent": {
      "id": "demo-agent",
      "name": "üéØ Agent de D√©mo (0 Mo)",
      "type": "demo",
      "size_mb": 0,
      "quality": "demo",
      "speed": "instant",
      "description": "Agent de d√©monstration ultra-l√©ger - Aucun t√©l√©chargement requis. Parfait pour tester l'interface ORION imm√©diatement !",
      "capabilities": ["demo", "instant-response", "code-examples", "no-download"],
      "min_ram_gb": 0,
      "prompt_format": null,
      "urls": {
        "base": null,
        "shards": null
      },
      "config": {
        "max_sequence_length": 0,
        "quantization": "none"
      },
      "optimization": {
        "strategy": "immediate_load",
        "quantization": "none",
        "sharding": false,
        "reasoning": "Agent de d√©monstration simul√© - aucun t√©l√©chargement, r√©ponses pr√©-d√©finies"
      }
    },
    "neural-router": {
      "id": "MobileBERT-uncased",
      "name": "MobileBERT Router",
      "type": "classification",
      "size_mb": 95,
      "quality": "ultra",
      "speed": "very-fast",
      "description": "Routeur neuronal ultra-rapide pour classification d'intention zero-shot",
      "capabilities": ["intent-classification", "zero-shot", "routing"],
      "min_ram_gb": 1,
      "prompt_format": null,
      "urls": {
        "base": "https://huggingface.co/google/mobilebert-uncased",
        "shards": null
      },
      "config": {
        "max_sequence_length": 512,
        "quantization": "int8"
      },
      "optimization": {
        "strategy": "immediate_load",
        "quantization": "none",
        "sharding": false,
        "reasoning": "Petit mod√®le (~95Mo) - chargement complet au d√©marrage pour routage instantan√©"
      }
    },
    "conversation-agent": {
      "id": "Phi-3-mini-4k-instruct-q4f16_1-MLC",
      "name": "Phi-3 Mini Instruct",
      "type": "causal-lm",
      "size_mb": 1800,
      "quality": "high",
      "speed": "fast",
      "description": "G√©n√©raliste polyvalent - Conversation, raisonnement, r√©daction (Microsoft)",
      "capabilities": ["chat", "reasoning", "writing", "code", "multilingual"],
      "min_ram_gb": 4,
      "prompt_format": {
        "system_prefix": "<|system|>\n",
        "user_prefix": "<|user|>\n",
        "assistant_prefix": "<|assistant|>\n",
        "eos_token": "<|end|>"
      },
      "urls": {
        "base": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct",
        "shards": null
      },
      "config": {
        "max_tokens": 4096,
        "temperature": 0.7,
        "top_p": 0.9
      },
      "optimization": {
        "strategy": "progressive_sharding",
        "quantization": "q4",
        "quantization_options": ["q2", "q3", "q4"],
        "sharding": true,
        "shard_size_mb": 100,
        "reasoning": "Mod√®le le plus utilis√© - sharding + quantification agressive pour chargement rapide"
      }
    },
    "code-agent": {
      "id": "CodeGemma-2b-it-q4f16_1-MLC",
      "name": "CodeGemma 2B Instruct",
      "type": "causal-lm",
      "size_mb": 1100,
      "quality": "high",
      "speed": "fast",
      "description": "Sp√©cialiste du code - G√©n√©ration, analyse et d√©bogage (Google)",
      "capabilities": ["code", "debugging", "code-generation", "code-explanation", "code-analysis"],
      "min_ram_gb": 4,
      "prompt_format": {
        "system_prefix": "<start_of_turn>system\n",
        "user_prefix": "<start_of_turn>user\n",
        "assistant_prefix": "<start_of_turn>model\n",
        "eos_token": "<end_of_turn>"
      },
      "urls": {
        "base": "https://huggingface.co/google/codegemma-2b-it",
        "shards": null
      },
      "config": {
        "max_tokens": 8192,
        "temperature": 0.2,
        "top_p": 0.95
      },
      "optimization": {
        "strategy": "progressive_sharding",
        "quantization": "q3",
        "quantization_options": ["q3", "q4"],
        "sharding": true,
        "shard_size_mb": 100,
        "reasoning": "Code sensible √† la pr√©cision - q3 minimum, sharding pour r√©activit√©"
      }
    },
    "vision-agent": {
      "id": "llava-v1.5-7b-q4f16_1-MLC",
      "name": "LLaVA v1.5 7B",
      "type": "vision-language",
      "size_mb": 4000,
      "quality": "ultra",
      "speed": "moderate",
      "description": "Analyste visuel - Compr√©hension d'images et questions-r√©ponses visuelles",
      "capabilities": ["chat", "vision", "image-understanding", "multimodal", "advanced-reasoning", "visual-qa"],
      "min_ram_gb": 6,
      "min_gpu": "RTX 3060 ou √©quivalent",
      "prompt_format": {
        "system_prefix": "### System:\n",
        "user_prefix": "### Human:\n",
        "assistant_prefix": "### Assistant:\n",
        "eos_token": "</s>"
      },
      "urls": {
        "base": "https://huggingface.co/liuhaotian/llava-v1.5-7b",
        "shards": null
      },
      "config": {
        "max_tokens": 4096,
        "temperature": 0.5,
        "top_p": 0.9
      },
      "optimization": {
        "strategy": "complete_on_demand",
        "quantization": "q4",
        "quantization_options": ["q3", "q4"],
        "sharding": false,
        "reasoning": "Mod√®le vision sensible - q4 minimum, chargement complet avec barre de progression. Encoder d'images doit √™tre charg√© enti√®rement, LLM peut √™tre shard√©"
      },
      "architecture": {
        "image_encoder": {
          "name": "CLIP ViT-L/14",
          "size_mb": 600,
          "load_priority": "high"
        },
        "llm": {
          "name": "Vicuna 7B",
          "size_mb": 3400,
          "load_priority": "progressive"
        }
      }
    },
    "logical-agent": {
      "id": "Llama-3.2-3B-Instruct-q4f16_1-MLC",
      "name": "Llama 3.2 3B",
      "type": "causal-lm",
      "size_mb": 1900,
      "quality": "very-high",
      "speed": "moderate",
      "description": "Mod√®le optimis√© pour le raisonnement logique",
      "capabilities": ["chat", "advanced-reasoning", "code", "multilingual", "long-context"],
      "min_ram_gb": 6,
      "prompt_format": {
        "system_prefix": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "user_prefix": "<|start_header_id|>user<|end_header_id|>\n",
        "assistant_prefix": "<|start_header_id|>assistant<|end_header_id|>\n",
        "eos_token": "<|eot_id|>"
      },
      "urls": {
        "base": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",
        "shards": null
      },
      "config": {
        "max_tokens": 8192,
        "temperature": 0.3,
        "top_p": 0.9
      }
    },
    "creative-agent": {
      "id": "Mistral-7B-Instruct-v0.2-q4f16_1-MLC",
      "name": "Mistral 7B Instruct",
      "type": "causal-lm",
      "size_mb": 4500,
      "quality": "ultra",
      "speed": "moderate",
      "description": "Mod√®le puissant pour la g√©n√©ration cr√©ative",
      "capabilities": ["chat", "expert-reasoning", "code", "multilingual", "long-context", "creative-writing"],
      "min_ram_gb": 6,
      "min_gpu": "RTX 3060 ou √©quivalent",
      "prompt_format": {
        "system_prefix": "[INST] ",
        "user_prefix": "",
        "assistant_prefix": "[/INST] ",
        "eos_token": "</s>"
      },
      "urls": {
        "base": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",
        "shards": null
      },
      "config": {
        "max_tokens": 8192,
        "temperature": 0.8,
        "top_p": 0.95
      }
    },
    "multilingual-agent": {
      "id": "Qwen2-1.5B-Instruct-q4f16_1-MLC",
      "name": "Qwen2 1.5B Instruct",
      "type": "causal-lm",
      "size_mb": 800,
      "quality": "high",
      "speed": "very-fast",
      "description": "Polyglotte - Traduction et assistance dans multiples langues (Alibaba)",
      "capabilities": ["chat", "translation", "multilingual", "reasoning"],
      "min_ram_gb": 3,
      "languages": ["en", "fr", "es", "de", "it", "pt", "zh", "ja", "ko", "ar", "ru", "hi", "th", "vi"],
      "prompt_format": {
        "system_prefix": "<|im_start|>system\n",
        "user_prefix": "<|im_start|>user\n",
        "assistant_prefix": "<|im_start|>assistant\n",
        "eos_token": "<|im_end|>"
      },
      "urls": {
        "base": "https://huggingface.co/Qwen/Qwen2-1.5B-Instruct",
        "shards": null
      },
      "config": {
        "max_tokens": 4096,
        "temperature": 0.7,
        "top_p": 0.9
      },
      "optimization": {
        "strategy": "progressive_sharding",
        "quantization": "q3",
        "quantization_options": ["q2", "q3", "q4"],
        "sharding": true,
        "shard_size_mb": 100,
        "reasoning": "Mod√®le compact multilingue - quantification agressive possible, valider performance √©gale entre langues"
      }
    },
    "speech-to-text-agent": {
      "id": "whisper-tiny",
      "name": "Whisper Tiny",
      "type": "speech-recognition",
      "size_mb": 150,
      "quality": "high",
      "speed": "very-fast",
      "description": "L'Oreille - Transcription audio ultra-rapide (OpenAI)",
      "capabilities": ["speech-to-text", "multilingual", "audio-translation"],
      "min_ram_gb": 2,
      "languages": ["en", "fr", "es", "de", "it", "pt", "zh", "ja", "ko", "ar", "ru", "hi", "th"],
      "prompt_format": null,
      "urls": {
        "base": "https://huggingface.co/openai/whisper-tiny",
        "shards": null
      },
      "config": {
        "sample_rate": 16000,
        "language": "auto",
        "task": "transcribe"
      },
      "optimization": {
        "strategy": "immediate_load",
        "quantization": "int8",
        "sharding": false,
        "reasoning": "Tr√®s petit mod√®le (~150Mo) - chargement complet et imm√©diat, pas besoin de sharding"
      }
    },
    "image-generation-agent": {
      "id": "stable-diffusion-2-1-base",
      "name": "Stable Diffusion 2.1",
      "type": "text-to-image",
      "size_mb": 1300,
      "quality": "ultra",
      "speed": "slow",
      "description": "L'Artiste - G√©n√©ration d'images √† partir de descriptions textuelles (Stability AI)",
      "capabilities": ["text-to-image", "image-generation", "creative"],
      "min_ram_gb": 4,
      "min_gpu": "GPU recommand√© (WebGPU)",
      "prompt_format": null,
      "urls": {
        "base": "https://huggingface.co/stabilityai/stable-diffusion-2-1-base",
        "shards": null
      },
      "config": {
        "num_inference_steps": 25,
        "guidance_scale": 7.5,
        "width": 512,
        "height": 512
      },
      "optimization": {
        "strategy": "complete_on_demand",
        "quantization": "q4",
        "sharding": false,
        "reasoning": "Mod√®le de diffusion tr√®s sensible - q4 minimum pour qualit√© d'image, chargement complet car UNet n√©cessite l'ensemble du mod√®le √† chaque √©tape"
      }
    },
    "hybrid-developer": {
      "id": "ORION-Dev-Polyglot-v1-q4f16_1-MLC",
      "name": "ORION Dev Polyglot v1",
      "type": "causal-lm",
      "size_mb": 1200,
      "quality": "very-high",
      "speed": "fast",
      "description": "Agent hybride fusionn√© (CodeGemma + Qwen2) - Expert en code ET multilingue",
      "capabilities": ["code", "code-generation", "code-explanation", "debugging", "multilingual", "chat", "reasoning", "translation"],
      "min_ram_gb": 4,
      "languages": ["en", "fr", "es", "de", "it", "pt", "zh", "ja", "ko", "ar", "ru"],
      "prompt_format": {
        "system_prefix": "<start_of_turn>system\n",
        "user_prefix": "<start_of_turn>user\n",
        "assistant_prefix": "<start_of_turn>model\n",
        "eos_token": "<end_of_turn>"
      },
      "urls": {
        "base": "/models/ORION-Dev-Polyglot-v1-q4/",
        "shards": [
          "/models/ORION-Dev-Polyglot-v1-q4/shard-00001-of-00012.bin",
          "/models/ORION-Dev-Polyglot-v1-q4/shard-00002-of-00012.bin",
          "/models/ORION-Dev-Polyglot-v1-q4/shard-00003-of-00012.bin",
          "/models/ORION-Dev-Polyglot-v1-q4/shard-00004-of-00012.bin",
          "/models/ORION-Dev-Polyglot-v1-q4/shard-00005-of-00012.bin",
          "/models/ORION-Dev-Polyglot-v1-q4/shard-00006-of-00012.bin",
          "/models/ORION-Dev-Polyglot-v1-q4/shard-00007-of-00012.bin",
          "/models/ORION-Dev-Polyglot-v1-q4/shard-00008-of-00012.bin",
          "/models/ORION-Dev-Polyglot-v1-q4/shard-00009-of-00012.bin",
          "/models/ORION-Dev-Polyglot-v1-q4/shard-00010-of-00012.bin",
          "/models/ORION-Dev-Polyglot-v1-q4/shard-00011-of-00012.bin",
          "/models/ORION-Dev-Polyglot-v1-q4/shard-00012-of-00012.bin"
        ]
      },
      "config": {
        "max_tokens": 6144,
        "temperature": 0.4,
        "top_p": 0.92
      },
      "optimization": {
        "strategy": "progressive_sharding",
        "quantization": "q4",
        "sharding": true,
        "shard_size_mb": 100,
        "reasoning": "Mod√®le fusionn√© custom - remplace 2 agents (CodeAgent + MultilingualAgent) par 1, √©conomise RAM"
      },
      "metadata": {
        "fusion": {
          "method": "slerp",
          "sources": [
            "google/codegemma-2b-it",
            "Qwen/Qwen2-1.5B-Instruct"
          ],
          "ratio": 0.4,
          "description": "60% CodeGemma (expertise code) + 40% Qwen2 (multilingue)",
          "created_by": "ORION Model Foundry",
          "created_at": "2025-10-23",
          "version": "1.0.0"
        }
      }
    }
  },
  "custom_models": {
    "orion-code-logic": {
      "id": "ORION-Code-Logic-v1-q4f16_1-MLC",
      "name": "ORION Code & Logic v1",
      "type": "causal-lm",
      "size_mb": 1500,
      "quality": "very-high",
      "speed": "fast",
      "description": "Agent hybride - Expert en code avec raisonnement logique avanc√©",
      "capabilities": ["code", "code-generation", "code-architecture", "algorithm-design", "debugging", "logical-reasoning", "step-by-step-analysis", "design-patterns"],
      "min_ram_gb": 5,
      "prompt_format": {
        "system_prefix": "<start_of_turn>system\n",
        "user_prefix": "<start_of_turn>user\n",
        "assistant_prefix": "<start_of_turn>model\n",
        "eos_token": "<end_of_turn>"
      },
      "urls": {
        "base": "/models/ORION-Code-Logic-v1-q4/",
        "shards": null
      },
      "config": {
        "max_tokens": 8192,
        "temperature": 0.3,
        "top_p": 0.92
      },
      "optimization": {
        "strategy": "progressive_sharding",
        "quantization": "q4",
        "sharding": true,
        "shard_size_mb": 150,
        "reasoning": "Mod√®le hybride - fusion 50/50 CodeGemma + Llama 3.2 pour code avec logique"
      },
      "metadata": {
        "fusion": {
          "method": "slerp",
          "sources": ["google/codegemma-2b-it", "meta-llama/Llama-3.2-3B-Instruct"],
          "ratio": 0.5,
          "description": "50% CodeGemma (code) + 50% Llama 3.2 (logique)",
          "created_by": "ORION Model Foundry",
          "created_at": "2025-10-24",
          "version": "1.0.0"
        }
      }
    },
    "orion-creative-multilingual": {
      "id": "ORION-Creative-Multilingual-v1-q4f16_1-MLC",
      "name": "ORION Creative & Multilingual v1",
      "type": "causal-lm",
      "size_mb": 2600,
      "quality": "ultra",
      "speed": "moderate",
      "description": "Agent hybride - Cr√©ativit√© exceptionnelle avec support multilingue",
      "capabilities": ["creative-writing", "storytelling", "brainstorming", "content-generation", "multilingual-creativity", "translation", "cultural-adaptation", "expert-reasoning"],
      "min_ram_gb": 6,
      "languages": ["en", "fr", "es", "de", "it", "pt", "zh", "ja", "ko", "ar", "ru"],
      "prompt_format": {
        "system_prefix": "[INST] ",
        "user_prefix": "",
        "assistant_prefix": "[/INST] ",
        "eos_token": "</s>"
      },
      "urls": {
        "base": "/models/ORION-Creative-Multilingual-v1-q4/",
        "shards": null
      },
      "config": {
        "max_tokens": 8192,
        "temperature": 0.8,
        "top_p": 0.95
      },
      "optimization": {
        "strategy": "progressive_sharding",
        "quantization": "q4",
        "sharding": true,
        "shard_size_mb": 200,
        "reasoning": "Mod√®le volumineux - fusion 70/30 Mistral + Qwen2 pour cr√©ativit√© multilingue"
      },
      "metadata": {
        "fusion": {
          "method": "slerp",
          "sources": ["mistralai/Mistral-7B-Instruct-v0.2", "Qwen/Qwen2-1.5B-Instruct"],
          "ratio": 0.3,
          "description": "70% Mistral (cr√©ativit√©) + 30% Qwen2 (multilingue)",
          "created_by": "ORION Model Foundry",
          "created_at": "2025-10-24",
          "version": "1.0.0"
        }
      }
    },
    "orion-vision-logic": {
      "id": "ORION-Vision-Logic-v1-q4f16_1-MLC",
      "name": "ORION Vision & Logic v1",
      "type": "vision-language",
      "size_mb": 3400,
      "quality": "ultra",
      "speed": "moderate",
      "description": "Agent hybride - Analyse visuelle avec raisonnement logique structur√©",
      "capabilities": ["vision", "image-understanding", "visual-qa", "object-detection", "scene-analysis", "logical-reasoning", "step-by-step-visual-analysis"],
      "min_ram_gb": 7,
      "prompt_format": {
        "system_prefix": "### System:\n",
        "user_prefix": "### Human:\n",
        "assistant_prefix": "### Assistant:\n",
        "eos_token": "</s>"
      },
      "urls": {
        "base": "/models/ORION-Vision-Logic-v1-q4/",
        "shards": null
      },
      "config": {
        "max_tokens": 4096,
        "temperature": 0.5,
        "top_p": 0.9
      },
      "architecture": {
        "vision_encoder": {
          "name": "CLIP ViT-L/14",
          "size_mb": 600,
          "load_priority": "high"
        },
        "llm": {
          "name": "Vicuna 7B + Llama 3.2 3B (fusionn√©)",
          "size_mb": 2800,
          "load_priority": "progressive"
        }
      },
      "optimization": {
        "strategy": "hybrid_loading",
        "quantization": "q4",
        "sharding": true,
        "vision_encoder_sharding": false,
        "llm_sharding": true,
        "llm_shard_size_mb": 200,
        "reasoning": "Vision sensible - encodeur complet, LLM fusionn√© avec sharding"
      },
      "metadata": {
        "fusion": {
          "method": "slerp",
          "sources": ["liuhaotian/llava-v1.5-7b", "meta-llama/Llama-3.2-3B-Instruct"],
          "ratio": 0.4,
          "description": "60% LLaVA (vision) + 40% Llama 3.2 (logique) - LLM uniquement",
          "created_by": "ORION Model Foundry",
          "created_at": "2025-10-24",
          "version": "1.0.0",
          "notes": "Encodeur vision de LLaVA conserv√© intact"
        }
      }
    }
  },
  "model_groups": {
    "lightweight": ["conversation-agent", "multilingual-agent", "speech-to-text-agent"],
    "standard": ["conversation-agent", "code-agent", "multilingual-agent"],
    "advanced": ["logical-agent", "creative-agent", "vision-agent"],
    "coding": ["code-agent", "hybrid-developer", "orion-code-logic", "conversation-agent"],
    "multimodal": ["vision-agent", "orion-vision-logic", "speech-to-text-agent", "conversation-agent"],
    "hybrid": ["hybrid-developer", "orion-code-logic", "orion-creative-multilingual", "orion-vision-logic"],
    "creative": ["creative-agent", "orion-creative-multilingual", "conversation-agent"],
    "orion-custom": ["orion-code-logic", "orion-creative-multilingual", "orion-vision-logic"]
  },
  "recommendations": {
    "low_memory": "conversation-agent",
    "balanced": "conversation-agent",
    "high_quality": "creative-agent",
    "coding": "hybrid-developer",
    "coding_specialist": "code-agent",
    "coding_with_logic": "orion-code-logic",
    "vision": "vision-agent",
    "vision_with_logic": "orion-vision-logic",
    "multilingual": "multilingual-agent",
    "creative_multilingual": "orion-creative-multilingual",
    "hybrid_coding": "hybrid-developer",
    "orion_ultimate_code": "orion-code-logic",
    "orion_ultimate_creative": "orion-creative-multilingual",
    "orion_ultimate_vision": "orion-vision-logic"
  }
}
