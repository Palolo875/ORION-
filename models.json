{
  "$schema": "./models.schema.json",
  "version": "1.0.0",
  "last_updated": "2025-10-22",
  "models": {
    "conversation-agent": {
      "id": "Phi-3-mini-4k-instruct-q4f16_1-MLC",
      "name": "Phi-3 Mini",
      "type": "causal-lm",
      "size_mb": 2048,
      "quality": "high",
      "speed": "fast",
      "description": "Modèle de conversation général de Microsoft",
      "capabilities": ["chat", "reasoning", "code", "multilingual"],
      "min_ram_gb": 4,
      "prompt_format": {
        "system_prefix": "<|system|>\n",
        "user_prefix": "<|user|>\n",
        "assistant_prefix": "<|assistant|>\n",
        "eos_token": "<|end|>"
      },
      "urls": {
        "base": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct",
        "shards": null
      },
      "config": {
        "max_tokens": 4096,
        "temperature": 0.7,
        "top_p": 0.9
      }
    },
    "code-agent": {
      "id": "CodeGemma-2b-q4f16_1-MLC",
      "name": "CodeGemma 2B",
      "type": "causal-lm",
      "size_mb": 1600,
      "quality": "high",
      "speed": "fast",
      "description": "Modèle spécialisé pour la programmation de Google",
      "capabilities": ["code", "debugging", "code-generation", "code-explanation"],
      "min_ram_gb": 4,
      "prompt_format": {
        "system_prefix": "<|system|>\n",
        "user_prefix": "<|user|>\n",
        "assistant_prefix": "<|assistant|>\n",
        "eos_token": "<|endoftext|>"
      },
      "urls": {
        "base": "https://huggingface.co/google/codegemma-2b",
        "shards": null
      },
      "config": {
        "max_tokens": 8192,
        "temperature": 0.2,
        "top_p": 0.95
      }
    },
    "vision-agent": {
      "id": "llava-1.5-7b-hf-q4f16_1-MLC",
      "name": "LLaVA 7B Vision",
      "type": "vision-language",
      "size_mb": 4200,
      "quality": "ultra",
      "speed": "moderate",
      "description": "Modèle multimodal pour analyse d'images",
      "capabilities": ["chat", "vision", "image-understanding", "multimodal", "advanced-reasoning"],
      "min_ram_gb": 6,
      "min_gpu": "RTX 3060 ou équivalent",
      "prompt_format": {
        "system_prefix": "### System:\n",
        "user_prefix": "### Human:\n",
        "assistant_prefix": "### Assistant:\n",
        "eos_token": "</s>"
      },
      "urls": {
        "base": "https://huggingface.co/liuhaotian/llava-v1.5-7b",
        "shards": null
      },
      "config": {
        "max_tokens": 4096,
        "temperature": 0.5,
        "top_p": 0.9
      }
    },
    "logical-agent": {
      "id": "Llama-3.2-3B-Instruct-q4f16_1-MLC",
      "name": "Llama 3.2 3B",
      "type": "causal-lm",
      "size_mb": 1900,
      "quality": "very-high",
      "speed": "moderate",
      "description": "Modèle optimisé pour le raisonnement logique",
      "capabilities": ["chat", "advanced-reasoning", "code", "multilingual", "long-context"],
      "min_ram_gb": 6,
      "prompt_format": {
        "system_prefix": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "user_prefix": "<|start_header_id|>user<|end_header_id|>\n",
        "assistant_prefix": "<|start_header_id|>assistant<|end_header_id|>\n",
        "eos_token": "<|eot_id|>"
      },
      "urls": {
        "base": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",
        "shards": null
      },
      "config": {
        "max_tokens": 8192,
        "temperature": 0.3,
        "top_p": 0.9
      }
    },
    "creative-agent": {
      "id": "Mistral-7B-Instruct-v0.2-q4f16_1-MLC",
      "name": "Mistral 7B Instruct",
      "type": "causal-lm",
      "size_mb": 4500,
      "quality": "ultra",
      "speed": "moderate",
      "description": "Modèle puissant pour la génération créative",
      "capabilities": ["chat", "expert-reasoning", "code", "multilingual", "long-context", "creative-writing"],
      "min_ram_gb": 6,
      "min_gpu": "RTX 3060 ou équivalent",
      "prompt_format": {
        "system_prefix": "[INST] ",
        "user_prefix": "",
        "assistant_prefix": "[/INST] ",
        "eos_token": "</s>"
      },
      "urls": {
        "base": "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2",
        "shards": null
      },
      "config": {
        "max_tokens": 8192,
        "temperature": 0.8,
        "top_p": 0.95
      }
    },
    "multilingual-agent": {
      "id": "gemma-2b-it-q4f16_1-MLC",
      "name": "Gemma 2B",
      "type": "causal-lm",
      "size_mb": 1500,
      "quality": "high",
      "speed": "fast",
      "description": "Modèle multilingue compact de Google",
      "capabilities": ["chat", "reasoning", "code", "multilingual"],
      "min_ram_gb": 4,
      "languages": ["en", "fr", "es", "de", "it", "pt", "zh", "ja", "ko", "ar"],
      "prompt_format": {
        "system_prefix": "<start_of_turn>system\n",
        "user_prefix": "<start_of_turn>user\n",
        "assistant_prefix": "<start_of_turn>model\n",
        "eos_token": "<end_of_turn>"
      },
      "urls": {
        "base": "https://huggingface.co/google/gemma-2b-it",
        "shards": null
      },
      "config": {
        "max_tokens": 8192,
        "temperature": 0.7,
        "top_p": 0.9
      }
    },
    "speech-to-text-agent": {
      "id": "whisper-base",
      "name": "Whisper Base",
      "type": "speech-recognition",
      "size_mb": 140,
      "quality": "high",
      "speed": "very-fast",
      "description": "Modèle de transcription audio d'OpenAI",
      "capabilities": ["speech-to-text", "multilingual", "translation"],
      "min_ram_gb": 2,
      "languages": ["en", "fr", "es", "de", "it", "pt", "zh", "ja", "ko", "ar", "ru"],
      "prompt_format": null,
      "urls": {
        "base": "https://huggingface.co/openai/whisper-base",
        "shards": null
      },
      "config": {
        "sample_rate": 16000,
        "language": "auto"
      }
    },
    "hybrid-developer": {
      "id": "ORION-Dev-Polyglot-v1-q4f16_1-MLC",
      "name": "ORION Dev Polyglot",
      "type": "causal-lm",
      "size_mb": 1200,
      "quality": "very-high",
      "speed": "fast",
      "description": "Agent hybride fusionné (CodeGemma + Qwen2) - Expert en code multilingue",
      "capabilities": ["code", "code-generation", "code-explanation", "debugging", "multilingual", "chat", "reasoning"],
      "min_ram_gb": 4,
      "languages": ["en", "fr", "es", "de", "it", "pt", "zh", "ja", "ko", "ar"],
      "prompt_format": {
        "system_prefix": "<|system|>\n",
        "user_prefix": "<|user|>\n",
        "assistant_prefix": "<|assistant|>\n",
        "eos_token": "<|end|>"
      },
      "urls": {
        "base": "/models/ORION-Dev-Polyglot-v1-q4f16_1/",
        "shards": null
      },
      "config": {
        "max_tokens": 4096,
        "temperature": 0.5,
        "top_p": 0.9
      },
      "metadata": {
        "fusion": {
          "method": "slerp",
          "sources": ["google/codegemma-2b", "Qwen/Qwen2-1.5B-Instruct"],
          "ratio": 0.4,
          "created_by": "ORION Model Foundry"
        }
      }
    }
  },
  "custom_models": {},
  "model_groups": {
    "lightweight": ["conversation-agent", "multilingual-agent", "speech-to-text-agent"],
    "standard": ["conversation-agent", "code-agent", "multilingual-agent"],
    "advanced": ["logical-agent", "creative-agent", "vision-agent"],
    "coding": ["code-agent", "hybrid-developer", "conversation-agent"],
    "multimodal": ["vision-agent", "speech-to-text-agent", "conversation-agent"],
    "hybrid": ["hybrid-developer"]
  },
  "recommendations": {
    "low_memory": "conversation-agent",
    "balanced": "conversation-agent",
    "high_quality": "creative-agent",
    "coding": "hybrid-developer",
    "coding_specialist": "code-agent",
    "vision": "vision-agent",
    "multilingual": "multilingual-agent",
    "hybrid_coding": "hybrid-developer"
  }
}
