# üß† Syst√®me de Comportement Humain Am√©lior√© pour ORION

## Vue d'ensemble

Ce syst√®me permet √† ORION de se comporter de mani√®re plus naturelle et humaine, **en conservant uniquement les avantages humains** tout en **r√©duisant drastiquement les erreurs et hallucinations**.

## üéØ Objectifs

### Avantages Humains Conserv√©s
- ‚úÖ **Honn√™tet√©** : Admet l'incertitude et l'ignorance
- ‚úÖ **Pens√©e visible** : Montre le processus de r√©flexion
- ‚úÖ **Auto-correction** : Se corrige en temps r√©el
- ‚úÖ **Demandes de clarification** : Pose des questions quand n√©cessaire
- ‚úÖ **Empathie cognitive** : Comprend le contexte et les nuances

### Erreurs Humaines √âlimin√©es
- ‚ùå **Hallucinations** : Syst√®me de d√©tection et pr√©vention
- ‚ùå **Biais de confirmation** : Validation crois√©e entre agents
- ‚ùå **Sur-confiance** : Calibration pr√©cise de la certitude
- ‚ùå **Invention de faits** : V√©rifications anti-hallucination
- ‚ùå **Manque de rigueur** : Score de confiance obligatoire

## üì¶ Composants

### 1. Module Core (`src/utils/humanBehavior.ts`)

Le module principal fournit :

```typescript
import {
  enrichWithHumanBehavior,
  calculateConfidence,
  detectPotentialHallucination,
  validateAgentConsensus,
  needsClarification,
} from '@/utils/humanBehavior';
```

#### Fonctions Principales

**`enrichWithHumanBehavior(response, query, config, context)`**
Enrichit une r√©ponse avec des comportements humains naturels.

```typescript
const enriched = enrichWithHumanBehavior(
  "Voici ma r√©ponse...",
  "Question de l'utilisateur",
  {
    enableThinking: true,
    enableUncertainty: true,
    confidenceThreshold: 0.7,
  },
  { hasMemory: true, agentConsensus: 0.85 }
);

console.log(enriched.confidence.score); // 0.85
console.log(enriched.thinking); // "Laissez-moi r√©fl√©chir √† cela..."
```

**`calculateConfidence(response, query, context)`**
Calcule un score de confiance bas√© sur plusieurs facteurs.

```typescript
const confidence = calculateConfidence(
  "Selon les donn√©es de 2020, la r√©ponse est X",
  "Question",
  { hasMemory: true }
);

// {
//   score: 0.82,
//   reasoning: "Score bas√© sur : bonnes bases factuelles, coh√©rence logique forte",
//   factors: {
//     factualBasis: 0.8,
//     logicalConsistency: 0.9,
//     domainExpertise: 0.7,
//     uncertaintyLevel: 0.2
//   }
// }
```

**`detectPotentialHallucination(response)`**
D√©tecte les hallucinations potentielles dans une r√©ponse.

```typescript
const check = detectPotentialHallucination(
  "En 2028, cet √©v√©nement s'est produit. Exactement 99.99% des gens..."
);

// {
//   likely: true,
//   warnings: [
//     "Mention de dates futures (possible hallucination)",
//     "Affirmations tr√®s sp√©cifiques sans source"
//   ],
//   confidence: 0.3
// }
```

**`validateAgentConsensus(responses)`**
Valide la coh√©rence entre plusieurs r√©ponses d'agents.

```typescript
const validation = validateAgentConsensus([
  { agent: 'logical', response: 'Solution A est optimale car X' },
  { agent: 'creative', response: 'Je propose A avec l\'approche Y' },
  { agent: 'critical', response: 'A semble correct mais attention √† Z' }
]);

// {
//   consensus: 0.75,
//   contradictions: [],
//   recommendations: []
// }
```

### 2. Hook React (`src/hooks/useHumanBehavior.ts`)

Hook principal pour int√©grer facilement les fonctionnalit√©s.

```typescript
import { useHumanBehavior } from '@/hooks/useHumanBehavior';

function MyComponent() {
  const {
    enrichResponse,
    checkHallucination,
    calculateResponseConfidence,
    validateMultipleAgents,
    lastConfidence,
  } = useHumanBehavior({
    config: {
      enableThinking: true,
      confidenceThreshold: 0.7,
    }
  });
  
  const handleResponse = (response: string, query: string) => {
    // Enrichir la r√©ponse
    const enriched = enrichResponse(response, query, { hasMemory: true });
    
    // V√©rifier les hallucinations
    const hallucinationCheck = checkHallucination(response);
    
    if (hallucinationCheck.likely) {
      console.warn("Possible hallucination d√©tect√©e!");
    }
    
    return enriched;
  };
}
```

#### Hooks Sp√©cialis√©s

**`useHallucinationDetector()`** - D√©tection d'hallucinations en temps r√©el
```typescript
const { checkHallucination, isResponseSafe } = useHallucinationDetector();

if (!isResponseSafe(response, 0.6)) {
  // Alerter l'utilisateur
}
```

**`useConfidenceTracking()`** - Suivi historique de la confiance
```typescript
const { 
  trackConfidence, 
  averageConfidence, 
  lowConfidenceCount 
} = useConfidenceTracking();

const confidence = trackConfidence(response, query);
console.log(`Confiance moyenne : ${averageConfidence * 100}%`);
```

### 3. Composants UI

**`ConfidenceIndicator`** - Affichage visuel du score de confiance

```tsx
import { ConfidenceIndicator } from '@/components/ConfidenceIndicator';

<ConfidenceIndicator
  confidence={confidence}
  showDetails={true}
  hallucinationWarnings={["Dates futures d√©tect√©es"]}
/>
```

**`ConfidenceBadge`** - Version compacte inline

```tsx
import { ConfidenceBadge } from '@/components/ConfidenceIndicator';

<p>
  R√©ponse <ConfidenceBadge confidence={confidence} />
</p>
```

## üîß Configuration des Agents

Les prompts syst√®me des agents ont √©t√© enrichis avec des instructions de comportement humain avanc√©.

### Agent Logique
```
COMPORTEMENT HUMAIN AVANC√â :
- Si tu n'as pas assez d'informations, ADMETS-LE
- Si plusieurs interpr√©tations sont possibles, MENTIONNE-LES
- N'invente JAMAIS de donn√©es - utilise "donn√©es non disponibles"
- Calibre ta certitude : "certain", "probable", "hypoth√©tique"
```

### Agent Cr√©atif
```
COMPORTEMENT HUMAIN AVANC√â :
- Distingue entre "id√©e sp√©culative" et "proposition concr√®te"
- Utilise "imagination" ou "hypoth√®se cr√©ative" pour cadrer tes id√©es
- Si une analogie peut √™tre mal comprise, PR√âCISE son sens
- Reste cr√©atif SANS inventer de faux faits
```

### Agent Critique
```
COMPORTEMENT HUMAIN AVANC√â :
- Distingue entre "risque prouv√©" et "risque hypoth√©tique"
- Si tu identifies une erreur factuelle, CITE pourquoi
- Sois critique mais HONN√äTE : si un argument est solide, reconnais-le
- √âvite les critiques bas√©es sur des suppositions non v√©rifi√©es
```

### Agent Synth√©tiseur
```
COMPORTEMENT HUMAIN AVANC√â :
- CALIBRE ta confiance : si les agents divergent, MENTIONNE-LE
- Si une information cl√© manque, DIS-LE
- Utilise : üü¢ Confiance √©lev√©e | üü° Confiance moyenne | üî¥ Incertitude
- Si tu d√©tectes une hallucination, ALERTE l'utilisateur
```

## üìä Syst√®me de Score de Confiance

### Facteurs de Calcul

| Facteur | Poids | Description |
|---------|-------|-------------|
| **Bases factuelles** | 30% | Pr√©sence de faits, chiffres, r√©f√©rences |
| **Coh√©rence logique** | 30% | Structure, absence de contradictions |
| **Expertise domaine** | 20% | Consensus entre agents |
| **Niveau certitude** | 20% | Absence de marqueurs d'incertitude |

### Interpr√©tation

- **üü¢ 0.8 - 1.0** : Confiance √©lev√©e - R√©ponse fiable
- **üü° 0.6 - 0.8** : Confiance moyenne - R√©ponse valide mais √† nuancer
- **üî¥ 0.0 - 0.6** : Incertitude - V√©rification recommand√©e

## üõ°Ô∏è Syst√®me Anti-Hallucination

### D√©tections Automatiques

1. **Dates futures** : D√©tecte les r√©f√©rences temporelles impossibles
2. **Affirmations sp√©cifiques sans source** : "Exactement 73.456%"
3. **Contradictions internes** : "C'est vrai... mais c'est faux"
4. **Noms propres invent√©s** : D√©tection heuristique
5. **Langage sur-confiant** : "Absolument certain" sans justification

### Actions Automatiques

Quand une hallucination est d√©tect√©e :
- ‚ö†Ô∏è Ajout d'un disclaimer visible
- üìâ R√©duction automatique du score de confiance
- üîî Alerte dans le composant UI
- üìù Logging pour analyse

## üß™ Tests

### Ex√©cution des Tests

```bash
npm run test -- src/utils/__tests__/humanBehavior.test.ts
```

### Couverture

- ‚úÖ D√©tection de clarification
- ‚úÖ Calcul de confiance
- ‚úÖ D√©tection d'hallucination
- ‚úÖ Validation consensus agents
- ‚úÖ Enrichissement de r√©ponses
- ‚úÖ G√©n√©ration de prompts

## üìà Int√©gration dans le Flow Existant

### Dans le LLM Worker

```typescript
import { generateHumanAwarePrompt } from '@/utils/humanBehavior';

// Enrichir le system prompt
const enrichedPrompt = generateHumanAwarePrompt(baseSystemPrompt, {
  enableThinking: true,
  enableUncertainty: true,
  confidenceThreshold: 0.7,
});
```

### Dans les Composants Chat

```typescript
import { useHumanBehavior } from '@/hooks/useHumanBehavior';
import { ConfidenceIndicator } from '@/components/ConfidenceIndicator';

function ChatMessage({ message }) {
  const { enrichResponse, checkHallucination } = useHumanBehavior();
  
  // Enrichir la r√©ponse
  const enriched = enrichResponse(message.text, message.query);
  
  // V√©rifier hallucinations
  const halluCheck = checkHallucination(message.text);
  
  return (
    <div>
      {enriched.thinking && <p className="italic">{enriched.thinking}</p>}
      <p>{enriched.response}</p>
      <ConfidenceIndicator 
        confidence={enriched.confidence}
        hallucinationWarnings={halluCheck.warnings}
      />
    </div>
  );
}
```

### Dans Multi-Agent Coordinator

```typescript
import { validateAgentConsensus } from '@/utils/humanBehavior';

// Apr√®s collecte des r√©ponses
const validation = validateAgentConsensus(agentResponses);

if (validation.consensus < 0.5) {
  console.warn('Faible consensus entre agents');
}

// Utiliser le consensus dans la synth√®se
const finalResponse = synthesize(
  agentResponses, 
  { consensus: validation.consensus }
);
```

## üé® Exemples d'Usage

### Exemple 1 : Question Simple

**Input** : "Quelle est la capitale de la France ?"

**Output** :
```
R√©ponse : "Paris est la capitale de la France."
Confiance : üü¢ 95% (Confiance √©lev√©e)
Facteurs :
  - Bases factuelles : 100%
  - Coh√©rence logique : 95%
  - Expertise : 90%
```

### Exemple 2 : Question Ambigu√´

**Input** : "Explique-moi √ßa"

**Output** :
```
Clarification n√©cessaire : "Votre question contient des pronoms ambigus. 
√Ä quoi faites-vous r√©f√©rence exactement ?"
```

### Exemple 3 : Hallucination D√©tect√©e

**Input** : Question complexe

**Output** :
```
‚ö†Ô∏è Note : Affirmations tr√®s sp√©cifiques sans source

R√©ponse : [...]

Confiance : üî¥ 45% (Incertitude)
‚ö†Ô∏è Cette r√©ponse pourrait contenir des informations incorrectes.
V√©rifiez les faits importants.
```

### Exemple 4 : D√©saccord Entre Agents

**Validation** :
```
Consensus : 40%
Contradictions :
  - D√©saccord entre agents sur la validit√© de certaines affirmations

Recommandations :
  - R√©solution de contradictions n√©cessaire dans la synth√®se finale
  - Consid√©rez demander plus de d√©tails √† l'utilisateur
```

## üöÄ Roadmap

### Phase 1 : Fondations (‚úÖ Compl√©t√©)
- [x] Module core humanBehavior
- [x] Hook React useHumanBehavior
- [x] Composants UI ConfidenceIndicator
- [x] Enrichissement des prompts agents
- [x] Tests unitaires

### Phase 2 : Int√©gration (üöß En cours)
- [ ] Int√©gration dans le LLM Worker
- [ ] Int√©gration dans Multi-Agent Coordinator
- [ ] Int√©gration dans les composants Chat
- [ ] Tests E2E

### Phase 3 : Optimisation (üìã Planifi√©)
- [ ] Machine learning pour am√©liorer la d√©tection d'hallucination
- [ ] Historique de confiance par utilisateur
- [ ] A/B testing des configurations
- [ ] M√©triques de performance

## üìù Notes de D√©veloppement

### Principes de Design

1. **Non-intrusif** : Le syst√®me enrichit sans modifier le flow existant
2. **Configurable** : Tous les comportements sont activables/d√©sactivables
3. **Transparent** : Scores et raisonnements visibles pour l'utilisateur
4. **Performant** : Calculs l√©gers, pas de latence ajout√©e significative

### Limitations Connues

- D√©tection d'hallucination bas√©e sur heuristiques (pas ML pour l'instant)
- Consensus agents bas√© sur analyse lexicale simple
- Pas de v√©rification externe de faits (future am√©lioration)

## ü§ù Contribution

Pour contribuer √† ce syst√®me :

1. Lire la documentation compl√®te
2. Ex√©cuter les tests : `npm test`
3. Ajouter des tests pour nouvelles fonctionnalit√©s
4. Documenter les changements dans ce fichier

## üìö R√©f√©rences

- [Calibrated Confidence in AI Systems](https://arxiv.org/abs/...)
- [Hallucination Detection in LLMs](https://arxiv.org/abs/...)
- [Human-AI Collaboration Best Practices](https://...)

---

**Derni√®re mise √† jour** : Octobre 2025  
**Mainteneur** : √âquipe ORION
