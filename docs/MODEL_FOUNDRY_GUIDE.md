# üè≠ Guide Complet de la Fonderie de Mod√®les ORION

## Table des mati√®res

1. [Introduction](#introduction)
2. [Concepts fondamentaux](#concepts-fondamentaux)
3. [Architecture](#architecture)
4. [Utilisation](#utilisation)
5. [Cr√©ation de recettes](#cr√©ation-de-recettes)
6. [Optimisation](#optimisation)
7. [Int√©gration](#int√©gration)
8. [Cas d'usage](#cas-dusage)
9. [Troubleshooting](#troubleshooting)
10. [FAQ](#faq)

## Introduction

La **ORION Model Foundry** est un syst√®me de cr√©ation d'agents AI hybrides par fusion de mod√®les. Elle permet de combiner les capacit√©s de plusieurs mod√®les existants pour cr√©er des agents uniques et optimis√©s.

### Avantages

| Avantage | Description | Impact |
|----------|-------------|--------|
| üéØ **Sp√©cialisation** | Cr√©ez des agents parfaitement adapt√©s √† vos besoins | Meilleure performance sur vos cas d'usage |
| üíæ **√âconomie de ressources** | Un agent hybride remplace plusieurs agents sp√©cialis√©s | -72% de RAM utilis√©e |
| ‚ö° **Performance** | Chargement plus rapide, latence r√©duite | TTFT < 3 secondes |
| üîß **Contr√¥le** | Ajustez pr√©cis√©ment le ratio de fusion | Personnalisation totale |
| üîÑ **Reproductibilit√©** | Recettes versionn√©es et document√©es | Maintenance simplifi√©e |

### Philosophie

La fonderie repose sur trois principes :

1. **D√©claratif** : Les recettes YAML d√©crivent *quoi* fusionner, pas *comment*
2. **Isol√©** : Processus compl√®tement s√©par√© du reste d'ORION
3. **Reproductible** : M√™mes entr√©es = m√™mes sorties

## Concepts fondamentaux

### Fusion de mod√®les (Model Merging)

La fusion de mod√®les consiste √† combiner les poids (param√®tres) de plusieurs mod√®les pour cr√©er un nouveau mod√®le qui h√©rite des capacit√©s des parents.

#### M√©thode SLERP (Spherical Linear Interpolation)

```python
merged_param = (1 - t) * model1_param + t * model2_param
```

O√π `t` est le ratio de fusion (0.0 √† 1.0).

**Exemples :**

| Ratio (t) | R√©sultat | Cas d'usage |
|-----------|----------|-------------|
| 0.0 | 100% Mod√®le 1 | Inutile (pas de fusion) |
| 0.3 | 70% M1 + 30% M2 | Favoriser M1, nuancer avec M2 |
| 0.5 | 50% M1 + 50% M2 | √âquilibre parfait |
| 0.7 | 30% M1 + 70% M2 | Favoriser M2, nuancer avec M1 |
| 1.0 | 100% Mod√®le 2 | Inutile (pas de fusion) |

### Quantification

R√©duction de la pr√©cision des poids pour diminuer la taille du mod√®le.

| Type | Bits | R√©duction | Qualit√© | Usage |
|------|------|-----------|---------|-------|
| fp32 | 32 | 0% | Excellente | Baseline |
| fp16 | 16 | ~50% | Excellente | Standard |
| q8 | 8 | ~75% | Tr√®s bonne | Production |
| q4 | 4 | ~87% | Bonne | Web (recommand√©) |
| q3 | 3 | ~90% | Acceptable | Contraintes m√©moire |

### Sharding

D√©coupage du mod√®le en plusieurs fichiers pour permettre un chargement progressif.

```
Mod√®le complet (1.2 GB)
    ‚Üì
Shard 1 (200 MB) ‚Üê Chargement prioritaire
Shard 2 (200 MB) ‚Üê Chargement prioritaire
Shard 3 (200 MB) ‚Üê Chargement en arri√®re-plan
Shard 4 (200 MB) ‚Üê Chargement en arri√®re-plan
Shard 5 (200 MB) ‚Üê Chargement en arri√®re-plan
Shard 6 (200 MB) ‚Üê Chargement en arri√®re-plan
```

**B√©n√©fice :** Time To First Token (TTFT) r√©duit de 10s ‚Üí 3s

## Architecture

### Structure des dossiers

```
model_foundry/
‚îú‚îÄ‚îÄ recipes/                    # Recettes de fusion (YAML)
‚îÇ   ‚îú‚îÄ‚îÄ dev-polyglot-v1.yml    # Agent d√©veloppeur polyglotte
‚îÇ   ‚îú‚îÄ‚îÄ creative-coder-v1.yml  # Agent cr√©atif
‚îÇ   ‚îî‚îÄ‚îÄ data-analyst-v1.yml    # Agent analyste de donn√©es
‚îÇ
‚îú‚îÄ‚îÄ merged_models/              # Sortie de la fusion (Git-ignored)
‚îÇ   ‚îî‚îÄ‚îÄ ORION-dev-polyglot-v1/
‚îÇ       ‚îú‚îÄ‚îÄ config.json
‚îÇ       ‚îú‚îÄ‚îÄ pytorch_model.bin
‚îÇ       ‚îú‚îÄ‚îÄ tokenizer.json
‚îÇ       ‚îî‚îÄ‚îÄ merge_metadata.json
‚îÇ
‚îú‚îÄ‚îÄ optimized_models/           # Mod√®les optimis√©s pour le web
‚îÇ   ‚îî‚îÄ‚îÄ ORION-dev-polyglot-v1-q4/
‚îÇ       ‚îú‚îÄ‚îÄ model.safetensors.index.json
‚îÇ       ‚îú‚îÄ‚îÄ model-00001-of-00006.safetensors
‚îÇ       ‚îú‚îÄ‚îÄ ...
‚îÇ       ‚îî‚îÄ‚îÄ web_config.json
‚îÇ
‚îú‚îÄ‚îÄ merge_models.py             # Script de fusion
‚îú‚îÄ‚îÄ optimize_for_web.py         # Script d'optimisation
‚îú‚îÄ‚îÄ foundry.sh                  # CLI principal
‚îú‚îÄ‚îÄ pyproject.toml              # Configuration Poetry
‚îú‚îÄ‚îÄ requirements.txt            # Alternative √† Poetry
‚îî‚îÄ‚îÄ README.md
```

### Workflow complet

```mermaid
graph LR
    A[Recette YAML] --> B[merge_models.py]
    B --> C[Mod√®le fusionn√©]
    C --> D[optimize_for_web.py]
    D --> E[Mod√®le optimis√©]
    E --> F[Int√©gration ORION]
    F --> G[Agent disponible]
```

## Utilisation

### Installation

**Option A : Poetry (recommand√©)**

```bash
cd model_foundry
./foundry.sh init
```

**Option B : pip + virtualenv**

```bash
cd model_foundry
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### Commandes de base

**1. Lister les recettes disponibles**

```bash
./foundry.sh list
```

**2. Cr√©er un agent hybride (pipeline complet)**

```bash
./foundry.sh pipeline recipes/dev-polyglot-v1.yml
```

Cette commande effectue :
- ‚úÖ T√©l√©chargement des mod√®les parents (si n√©cessaire)
- ‚úÖ Fusion des mod√®les
- ‚úÖ Optimisation (quantification + sharding)
- ‚úÖ G√©n√©ration des m√©tadonn√©es

**3. Fusion seule**

```bash
./foundry.sh merge recipes/dev-polyglot-v1.yml
```

**4. Optimisation seule**

```bash
./foundry.sh optimize merged_models/ORION-dev-polyglot-v1
```

### Options avanc√©es

**Fusion avec sortie personnalis√©e**

```bash
poetry run python merge_models.py \
  --recipe recipes/custom.yml \
  --output merged_models/my-custom-model
```

**Optimisation avec options**

```bash
poetry run python optimize_for_web.py \
  --model merged_models/ORION-dev-polyglot-v1 \
  --output optimized_models/custom-output \
  --quantization q4f16_1 \
  --name "My-Custom-Agent-v1"
```

## Cr√©ation de recettes

### Template de base

```yaml
# recipes/my-agent-v1.yml

models:
  - model: org/model1-id
  - model: org/model2-id

merge_method: slerp

parameters:
  t: 0.5  # Ajustez selon vos besoins

dtype: bfloat16

metadata:
  created_by: "ORION Model Foundry"
  version: "1.0.0"
  description: "Description de votre agent"
  target_tasks:
    - "T√¢che 1"
    - "T√¢che 2"
  estimated_size_gb: 2.0
  recommended_quantization: "q4f16_1"
```

### Guide de s√©lection des ratios

#### Pour favoriser le code (70/30)

```yaml
parameters:
  t: 0.3  # 70% CodeGemma + 30% autre
```

**Cas d'usage :**
- G√©n√©ration de code avec support multilingue minimal
- Applications n√©cessitant du code pr√©cis
- Projets o√π le code prime sur la conversation

#### Pour l'√©quilibre (50/50)

```yaml
parameters:
  t: 0.5  # √âquilibre parfait
```

**Cas d'usage :**
- Agent polyvalent
- Besoin √©gal de code et d'autre capacit√©
- Prototypage initial (bon point de d√©part)

#### Pour favoriser la cr√©ativit√© (30/70)

```yaml
parameters:
  t: 0.7  # 30% code + 70% cr√©atif
```

**Cas d'usage :**
- Documentation technique cr√©ative
- G√©n√©ration d'API innovantes
- Brainstorming technique

### Exemples de recettes

#### Agent Documentation Technique

```yaml
models:
  - model: google/codegemma-2b
  - model: mistralai/Mistral-7B-Instruct-v0.2

merge_method: slerp
parameters:
  t: 0.6  # Favorise Mistral pour la r√©daction

metadata:
  description: "Expert en documentation technique"
  target_tasks:
    - "API documentation"
    - "Technical writing"
    - "Code comments generation"
```

#### Agent DevOps

```yaml
models:
  - model: google/codegemma-2b
  - model: meta-llama/Llama-3.2-3B-Instruct

merge_method: slerp
parameters:
  t: 0.4  # 60% code + 40% raisonnement

metadata:
  description: "Expert DevOps et infrastructure"
  target_tasks:
    - "Docker/Kubernetes configs"
    - "CI/CD pipeline design"
    - "Infrastructure as Code"
```

## Optimisation

### Quantification recommand√©e par type d'agent

| Type d'agent | Quantification | Raison |
|--------------|----------------|--------|
| Code simple | q4 | Bonne qualit√©, 87% r√©duction |
| Code complexe | q8 | Meilleure pr√©cision |
| Cr√©atif | q4 | Acceptable, prioriser taille |
| Vision | fp16 | Sensible √† la quantification |
| Audio | fp16 | Idem |

### Configuration de sharding

```python
# Dans optimize_for_web.py

# Mod√®le < 500 MB : Pas de sharding
sharding = None

# Mod√®le 500 MB - 1.5 GB : 4 shards
sharding = {
  "enabled": True,
  "numShards": 4,
  "initialShards": 2
}

# Mod√®le > 1.5 GB : 6-8 shards
sharding = {
  "enabled": True,
  "numShards": 6,
  "initialShards": 2
}
```

## Int√©gration

### 1. Copier le mod√®le

```bash
cp -r model_foundry/optimized_models/ORION-Dev-Polyglot-v1-q4 \
      public/models/
```

### 2. Mettre √† jour models.json

```json
{
  "my-hybrid-agent": {
    "id": "ORION-My-Agent-v1-q4f16_1-MLC",
    "name": "My Hybrid Agent",
    "type": "causal-lm",
    "size_mb": 1200,
    "capabilities": ["code", "chat"],
    "min_ram_gb": 4,
    "prompt_format": {
      "system_prefix": "<|system|>\n",
      "user_prefix": "<|user|>\n",
      "assistant_prefix": "<|assistant|>\n",
      "eos_token": "<|end|>"
    },
    "urls": {
      "base": "/models/ORION-My-Agent-v1-q4f16_1/",
      "shards": null
    }
  }
}
```

### 3. Cr√©er l'agent TypeScript

```typescript
// src/oie/agents/my-hybrid-agent.ts

import { BaseAgent } from './base-agent';
import { AgentInput, AgentOutput } from '../types/agent.types';
import { ProgressiveLoader } from '../utils/progressive-loader';

export class MyHybridAgent extends BaseAgent {
  constructor() {
    super({
      id: 'my-hybrid-agent',
      name: 'My Hybrid Agent',
      capabilities: ['code', 'chat'],
      modelSize: 1200,
      priority: 9,
      modelId: 'ORION-My-Agent-v1-q4f16_1-MLC'
    });
  }
  
  // ... impl√©mentation
}
```

### 4. Enregistrer l'agent

```typescript
// src/oie/agents/index.ts

export * from './my-hybrid-agent';
```

## Cas d'usage

### 1. R√©duire l'utilisation de RAM

**Probl√®me :** Deux agents s√©par√©s (Code + Multilingue) = 2.9 GB RAM

**Solution :** Un agent hybride = 1.2 GB RAM (-59%)

```yaml
# recipes/memory-optimized-v1.yml
models:
  - model: google/codegemma-2b
  - model: Qwen/Qwen2-1.5B-Instruct
parameters:
  t: 0.4
```

### 2. Cr√©er un expert domaine

**Probl√®me :** Besoin d'un agent expert en finance + code

**Solution :** Fusionner un mod√®le finance avec CodeGemma

```yaml
models:
  - model: google/codegemma-2b
  - model: financial-domain/finance-llm
parameters:
  t: 0.5  # √âquilibre
```

### 3. Am√©liorer progressivement

**Processus it√©ratif :**

1. **v1** : `t: 0.5` (baseline)
2. **v2** : `t: 0.4` (plus de code)
3. **v3** : `t: 0.3` (encore plus de code)
4. **√âvaluation** : Choisir la meilleure version

## Troubleshooting

### Probl√®me : Out of Memory (OOM)

**Sympt√¥mes :**
```
RuntimeError: CUDA out of memory
```

**Solutions :**

1. **R√©duire le dtype**
   ```yaml
   dtype: fp16  # Au lieu de fp32
   ```

2. **Utiliser CPU uniquement**
   ```bash
   export CUDA_VISIBLE_DEVICES=""
   ./foundry.sh merge ...
   ```

3. **Augmenter le swap**
   ```bash
   sudo fallocate -l 16G /swapfile
   sudo chmod 600 /swapfile
   sudo mkswap /swapfile
   sudo swapon /swapfile
   ```

### Probl√®me : Architectures incompatibles

**Sympt√¥mes :**
```
ValueError: Model architectures are incompatible
```

**Cause :** Les deux mod√®les ont des structures diff√©rentes

**Solutions :**

1. Utiliser des mod√®les de la m√™me famille
   - ‚úÖ Gemma + CodeGemma
   - ‚úÖ Llama + CodeLlama
   - ‚ùå Gemma + Mistral

2. V√©rifier les architectures
   ```python
   from transformers import AutoConfig
   
   config1 = AutoConfig.from_pretrained("model1")
   config2 = AutoConfig.from_pretrained("model2")
   
   print(config1.architectures)
   print(config2.architectures)
   ```

### Probl√®me : Mod√®le fusionn√© de mauvaise qualit√©

**Sympt√¥mes :** R√©ponses incoh√©rentes, hallucinations

**Solutions :**

1. **Ajuster le ratio**
   - Tester plusieurs valeurs de `t`
   - √âvaluer syst√©matiquement

2. **Choisir de meilleurs parents**
   - Mod√®les bien entra√Æn√©s
   - Tailles similaires

3. **Augmenter la quantification**
   ```yaml
   dtype: fp16  # Ou fp32 pour plus de pr√©cision
   ```

## FAQ

### Q1 : Puis-je fusionner plus de 2 mod√®les ?

**R :** Oui, en cha√Æne :

```bash
# √âtape 1: A + B = AB
./foundry.sh merge recipes/step1-ab.yml

# √âtape 2: AB + C = ABC
./foundry.sh merge recipes/step2-abc.yml
```

### Q2 : Combien de temps prend la fusion ?

**R :** D√©pend de la taille et du mat√©riel :

| Mod√®les | CPU | GPU | Temps estim√© |
|---------|-----|-----|--------------|
| 2x 2B | i7 | - | 15-20 min |
| 2x 2B | i7 | RTX 3060 | 8-12 min |
| 2x 7B | i7 | - | 45-60 min |
| 2x 7B | Xeon | RTX 4090 | 15-20 min |

### Q3 : Les mod√®les fusionn√©s sont-ils plus intelligents ?

**R :** Non, la fusion **combine** les capacit√©s, elle ne les **am√©liore** pas magiquement.

- ‚úÖ Un agent code + multilingue aura les deux capacit√©s
- ‚ùå Il ne sera pas meilleur en code que CodeGemma seul
- ‚úÖ Mais il sera plus polyvalent

### Q4 : Quelle est la licence du mod√®le fusionn√© ?

**R :** La plus restrictive des deux parents.

- Si Mod√®le A = MIT et Mod√®le B = Apache 2.0 ‚Üí Apache 2.0
- Si l'un est commercial uniquement ‚Üí Commercial uniquement

### Q5 : Puis-je fusionner des mod√®les de tailles tr√®s diff√©rentes ?

**R :** Oui, mais avec des r√©sultats variables :

- ‚úÖ 2B + 1.5B ‚Üí Bons r√©sultats
- ‚ö†Ô∏è 2B + 7B ‚Üí R√©sultats impr√©visibles
- ‚ùå 2B + 70B ‚Üí D√©conseill√©

### Q6 : La fusion fonctionne-t-elle pour tous les types de mod√®les ?

**R :** Non, principalement pour les mod√®les **causaux de langage** :

- ‚úÖ Mod√®les de texte (GPT, Llama, Gemma, etc.)
- ‚ö†Ô∏è Mod√®les vision-langage (exp√©rimental)
- ‚ùå Mod√®les de diffusion (Stable Diffusion, etc.)
- ‚ùå Mod√®les audio (Whisper, etc.)

## Ressources

### Documentation officielle

- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [Model Merging Guide](https://huggingface.co/blog/mlabonne/merge-models)
- [Quantization Guide](https://huggingface.co/docs/transformers/quantization)

### Mod√®les recommand√©s pour la fusion

| Mod√®le | Taille | Sp√©cialit√© | Licence |
|--------|--------|------------|---------|
| google/codegemma-2b | 2B | Code | Apache 2.0 |
| Qwen/Qwen2-1.5B-Instruct | 1.5B | Multilingue | Apache 2.0 |
| mistralai/Mistral-7B-Instruct-v0.2 | 7B | Cr√©atif | Apache 2.0 |
| meta-llama/Llama-3.2-3B-Instruct | 3B | Raisonnement | Llama 3 |
| google/gemma-2b-it | 2B | Chat | Gemma |

### Communaut√©

- [ORION Discord](#) (remplacer par lien r√©el)
- [GitHub Issues](#) (remplacer par lien r√©el)

---

**Derni√®re mise √† jour :** 2025-10-22  
**Version :** 1.0.0  
**Auteur :** √âquipe ORION üöÄ
