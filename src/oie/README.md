# Orion Inference Engine (OIE)

## ğŸ“‹ Vue d'ensemble

L'**Orion Inference Engine (OIE)** est un systÃ¨me d'orchestration intelligent pour les agents IA spÃ©cialisÃ©s dans ORION. Il gÃ¨re automatiquement :

- ğŸ§  **Routage intelligent** : SÃ©lectionne l'agent appropriÃ© selon la requÃªte
- ğŸ’¾ **Gestion de mÃ©moire** : Cache LRU pour optimiser les ressources
- ğŸ”„ **Chargement dynamique** : Les agents sont chargÃ©s Ã  la demande
- ğŸ¯ **Agents spÃ©cialisÃ©s** : Code, Vision, Conversation, Logique

## ğŸ—ï¸ Architecture

```
src/oie/
â”œâ”€â”€ core/              # Moteur principal
â”‚   â””â”€â”€ engine.ts      # OrionInferenceEngine
â”œâ”€â”€ agents/            # Agents spÃ©cialisÃ©s
â”‚   â”œâ”€â”€ base-agent.ts
â”‚   â”œâ”€â”€ conversation-agent.ts
â”‚   â”œâ”€â”€ code-agent.ts
â”‚   â”œâ”€â”€ vision-agent.ts
â”‚   â””â”€â”€ logical-agent.ts
â”œâ”€â”€ router/            # Routage intelligent
â”‚   â””â”€â”€ simple-router.ts
â”œâ”€â”€ cache/             # Gestion mÃ©moire
â”‚   â”œâ”€â”€ lru-cache.ts
â”‚   â””â”€â”€ cache-manager.ts
â””â”€â”€ types/             # DÃ©finitions TypeScript
```

## ğŸš€ Utilisation

### Avec le hook React (RecommandÃ©)

```typescript
import { useOIE } from '@/hooks/useOIE';

function MyComponent() {
  const { isReady, isProcessing, ask, error } = useOIE({
    maxMemoryMB: 8000,
    maxAgentsInMemory: 2,
    enableVision: true,
    enableCode: true,
  });

  const handleQuery = async () => {
    if (!isReady) return;
    
    try {
      const response = await ask("Comment crÃ©er une fonction en JavaScript ?");
      console.log(response.content);
    } catch (err) {
      console.error(err);
    }
  };

  return (
    <div>
      {!isReady && <div>Chargement du moteur...</div>}
      {error && <div>Erreur: {error}</div>}
      <button onClick={handleQuery} disabled={isProcessing || !isReady}>
        Poser une question
      </button>
    </div>
  );
}
```

### Utilisation directe (AvancÃ©)

```typescript
import { OrionInferenceEngine } from '@/oie';

// Initialisation
const engine = new OrionInferenceEngine({
  maxMemoryMB: 8000,
  maxAgentsInMemory: 2,
  enableVision: true,
  enableCode: true,
});

await engine.initialize();

// RequÃªte simple
const response = await engine.infer("Bonjour !");
console.log(response.content);

// RequÃªte avec options
const codeResponse = await engine.infer(
  "Ã‰cris une fonction pour trier un tableau",
  {
    temperature: 0.3,
    maxTokens: 2000,
    conversationHistory: [
      { role: 'user', content: 'PrÃ©cÃ©dent message' },
      { role: 'assistant', content: 'PrÃ©cÃ©dente rÃ©ponse' }
    ]
  }
);

// Forcer un agent spÃ©cifique
const logicalResponse = await engine.infer(
  "Analyse ce problÃ¨me",
  { forceAgent: 'logical-agent' }
);

// Cleanup
await engine.shutdown();
```

## ğŸ¤– Agents disponibles

### 1. **ConversationAgent** (`conversation-agent`)
- **ModÃ¨le** : Phi-3-mini-4k-instruct
- **Taille** : ~2GB
- **CapacitÃ©s** : Conversation gÃ©nÃ©rale, Ã©criture crÃ©ative
- **Usage** : Agent par dÃ©faut pour le dialogue

### 2. **CodeAgent** (`code-agent`)
- **ModÃ¨le** : CodeGemma-2B
- **Taille** : ~1.6GB
- **CapacitÃ©s** : GÃ©nÃ©ration de code, explication, dÃ©bogage
- **Usage** : Automatiquement sÃ©lectionnÃ© pour les questions de programmation

### 3. **VisionAgent** (`vision-agent`)
- **ModÃ¨le** : Phi-3-Vision
- **Taille** : ~2.4GB
- **CapacitÃ©s** : Analyse d'images, description visuelle
- **Usage** : Automatiquement sÃ©lectionnÃ© quand des images sont prÃ©sentes

### 4. **LogicalAgent** (`logical-agent`)
- **ModÃ¨le** : Phi-3-mini-4k-instruct
- **Taille** : ~2GB
- **CapacitÃ©s** : Analyse logique, dÃ©composition structurÃ©e
- **Usage** : Questions nÃ©cessitant du raisonnement rigoureux

## ğŸ“Š Gestion de mÃ©moire

L'OIE utilise un **cache LRU (Least Recently Used)** pour optimiser l'utilisation de la RAM :

```typescript
// Configuration du cache
const engine = new OrionInferenceEngine({
  maxMemoryMB: 8000,        // MÃ©moire maximale
  maxAgentsInMemory: 2,     // Nombre max d'agents chargÃ©s simultanÃ©ment
});

// Statistiques du cache
const stats = engine.getStats();
console.log(stats);
// {
//   agentsLoaded: 2,
//   totalMemoryMB: 3648,
//   maxMemoryMB: 8000,
//   memoryUsagePercent: 45.6,
//   agents: [...]
// }
```

## ğŸ§­ Routage automatique

Le routeur dÃ©tecte automatiquement l'intention :

| Mots-clÃ©s | Agent sÃ©lectionnÃ© |
|-----------|------------------|
| `code`, `fonction`, `script`, `programme` | CodeAgent |
| `image`, `photo`, `analyser image` | VisionAgent |
| `analyse`, `logique`, `Ã©tape` | LogicalAgent |
| Par dÃ©faut | ConversationAgent |

## ğŸ”§ Configuration avancÃ©e

### DÃ©sactiver certains agents

```typescript
const engine = new OrionInferenceEngine({
  enableVision: false,  // DÃ©sactiver l'agent vision
  enableCode: false,    // DÃ©sactiver l'agent code
});
```

### Forcer un agent spÃ©cifique

```typescript
const response = await engine.infer("Question", {
  forceAgent: 'code-agent'  // Force l'utilisation de CodeAgent
});
```

### Avec images

```typescript
const response = await engine.infer("DÃ©cris cette image", {
  images: [
    { 
      content: 'data:image/png;base64,...',
      type: 'image/png'
    }
  ]
});
```

## ğŸ¯ Exemple complet

```typescript
import { OrionInferenceEngine } from '@/oie';

async function demo() {
  // 1. Initialisation
  const engine = new OrionInferenceEngine({
    maxMemoryMB: 8000,
    maxAgentsInMemory: 2,
  });
  
  await engine.initialize();
  console.log('Agents disponibles:', engine.getAvailableAgents());
  
  // 2. Questions variÃ©es
  const responses = await Promise.all([
    engine.infer("Ã‰cris une fonction JavaScript pour calculer la factorielle"),
    engine.infer("Quelle est la capitale de la France ?"),
    engine.infer("Analyse logiquement: pourquoi le ciel est bleu ?"),
  ]);
  
  responses.forEach((res, i) => {
    console.log(`Question ${i + 1}:`);
    console.log(`Agent: ${res.agentId}`);
    console.log(`RÃ©ponse: ${res.content}`);
    console.log(`Temps: ${res.processingTime}ms`);
  });
  
  // 3. Statistiques
  console.log('Stats:', engine.getStats());
  
  // 4. Cleanup
  await engine.shutdown();
}

demo();
```

## ğŸ“ Notes importantes

1. **Premier chargement** : Le premier appel Ã  un agent prend du temps (tÃ©lÃ©chargement + initialisation)
2. **Cache persistant** : Les modÃ¨les sont mis en cache navigateur aprÃ¨s le premier tÃ©lÃ©chargement
3. **MÃ©moire** : Surveillez l'utilisation RAM avec `getStats()`
4. **Fallback** : En cas d'erreur, l'OIE tente automatiquement le ConversationAgent

## ğŸ› Debugging

```typescript
// Activer les logs dÃ©taillÃ©s
const engine = new OrionInferenceEngine({...});
await engine.initialize();

// Les logs apparaÃ®tront dans la console :
// [OIE] ğŸš€ Initialisation...
// [SimpleRouter] Agent enregistrÃ©: Agent Conversation
// [OIE] âœ… Moteur prÃªt
// [OIE] ğŸ“¥ RequÃªte reÃ§ue: "..."
// [OIE] ğŸ§­ Routage: code-agent (confiance: 85%)
// [CacheManager] Miss: code-agent - Chargement...
// [CodeAgent] Chargement du modÃ¨le...
// [OIE] âœ… RÃ©ponse gÃ©nÃ©rÃ©e en 1234ms
```

## ğŸ”® Roadmap

- [ ] Agent Multilingue (Qwen2)
- [ ] Agent Whisper (Speech-to-Text)
- [ ] Routeur neuronal (MobileBERT)
- [ ] Streaming des rÃ©ponses
- [ ] Benchmark automatique
- [ ] MÃ©triques de performance

## ğŸ“š Ressources

- [Documentation WebLLM](https://webllm.mlc.ai/)
- [Configuration des modÃ¨les](/src/config/models.ts)
- [Configuration des agents](/src/config/agents.ts)
