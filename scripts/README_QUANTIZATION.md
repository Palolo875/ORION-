# Pipeline de Quantification ORION

Ce dossier contient les outils pour quantifier et optimiser les mod√®les d'IA utilis√©s par ORION.

## üìã Pr√©requis

### Installation des d√©pendances Python

```bash
pip install optimum[onnxruntime] onnx transformers torch
```

### Configuration syst√®me

- **RAM**: Minimum 16 GB recommand√© pour quantifier des mod√®les de 2-4 GB
- **Espace disque**: Au moins 3x la taille du mod√®le original
- **CPU**: AVX512 recommand√© pour de meilleures performances (optionnel)

## üöÄ Utilisation

### Quantification Standard (Q4)

La quantification Q4 est le meilleur √©quilibre entre taille et qualit√©:

```bash
python scripts/quantize-model.py \
  --model microsoft/phi-3-mini-4k-instruct \
  --output models/phi-3-q4 \
  --test
```

**R√©sultat attendu:**
- Taille originale: ~7 GB (FP32)
- Taille ONNX: ~3.5 GB (FP16)
- Taille Q4: ~1.8 GB
- Compression: ~50%

### Quantification Agressive (Q2)

Pour une taille minimale au prix d'une l√©g√®re perte de qualit√©:

```bash
python scripts/quantize-model.py \
  --model microsoft/phi-3-mini-4k-instruct \
  --output models/phi-3-q2 \
  --level q2 \
  --test
```

**R√©sultat attendu:**
- Taille Q2: ~900 MB
- Compression: ~75%
- ‚ö†Ô∏è L√©g√®re d√©gradation possible de la qualit√©

### Quantification Interm√©diaire (Q3)

Compromis entre Q4 et Q2:

```bash
python scripts/quantize-model.py \
  --model microsoft/phi-3-mini-4k-instruct \
  --output models/phi-3-q3 \
  --level q3
```

## üìä Niveaux de Quantification

| Niveau | Taille | Qualit√© | Vitesse | Usage recommand√© |
|--------|--------|---------|---------|------------------|
| Q4     | ~1.8 GB | Excellente | Rapide | Production standard |
| Q3     | ~1.2 GB | Bonne | Tr√®s rapide | √âquilibre taille/qualit√© |
| Q2     | ~900 MB | Acceptable | Tr√®s rapide | Appareils limit√©s |

## üîß Options Avanc√©es

### Sans AVX512 (pour compatibilit√© ARM)

```bash
python scripts/quantize-model.py \
  --model microsoft/phi-3-mini-4k-instruct \
  --output models/phi-3-q4 \
  --no-avx512
```

### Batch Quantification

Pour quantifier plusieurs mod√®les:

```bash
# Cr√©ez un script batch
#!/bin/bash

models=(
  "microsoft/phi-3-mini-4k-instruct"
  "meta-llama/Llama-2-7b-chat-hf"
  "mistralai/Mistral-7B-Instruct-v0.2"
)

for model in "${models[@]}"; do
  model_name=$(basename $model)
  python scripts/quantize-model.py \
    --model "$model" \
    --output "models/${model_name}-q4" \
    --level q4
done
```

## üìÅ Structure de Sortie

Apr√®s quantification, le dossier de sortie contient:

```
models/phi-3-q4/
‚îú‚îÄ‚îÄ onnx/                    # Mod√®le ONNX interm√©diaire
‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îú‚îÄ‚îÄ model.onnx
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ quantized/               # Mod√®le quantifi√© (√Ä UTILISER)
‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îú‚îÄ‚îÄ model_quantized.onnx
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_config.json
‚îÇ   ‚îî‚îÄ‚îÄ special_tokens_map.json
‚îî‚îÄ‚îÄ metadata.json            # M√©tadonn√©es de quantification
```

## üåê H√©bergement des Mod√®les

### Option 1: Hugging Face Hub

```bash
# Installer huggingface_hub
pip install huggingface_hub

# Upload du mod√®le
huggingface-cli upload \
  votre-username/phi-3-q4-orion \
  models/phi-3-q4/quantized \
  --repo-type model
```

### Option 2: Serveur Personnel

H√©bergez le dossier `quantized/` sur votre serveur web:

```nginx
# Configuration Nginx exemple
location /models/ {
    root /var/www/orion;
    add_header Access-Control-Allow-Origin *;
    add_header Cache-Control "public, max-age=31536000";
}
```

### Option 3: CDN (Vercel, Netlify)

Uploadez le dossier `quantized/` dans votre d√©p√¥t et configurez le CDN.

## üîó Utilisation dans ORION

Une fois le mod√®le quantifi√© et h√©berg√©, modifiez votre agent:

```typescript
// src/oie/agents/conversation-agent.ts
export class ConversationAgent extends BaseAgent {
  constructor() {
    super({
      id: 'conversation-agent',
      name: 'Agent Conversation',
      capabilities: ['conversation'],
      modelSize: 1800, // Taille en MB
      priority: 10,
      // Changez cette URL vers votre mod√®le h√©berg√©
      modelId: 'https://votre-cdn.com/models/phi-3-q4'
    });
  }
}
```

## ‚úÖ Validation de la Qualit√©

### Test Automatique

Le script peut tester automatiquement le mod√®le:

```bash
python scripts/quantize-model.py \
  --model microsoft/phi-3-mini-4k-instruct \
  --output models/phi-3-q4 \
  --test
```

### Test Manuel

```python
from optimum.onnxruntime import ORTModelForCausalLM
from transformers import AutoTokenizer

model_path = "models/phi-3-q4/quantized"
model = ORTModelForCausalLM.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Test de g√©n√©ration
prompt = "Explique-moi la quantification de mod√®les IA"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
print(tokenizer.decode(outputs[0]))
```

### Benchmarks Recommand√©s

Pour √©valuer objectivement la qualit√©:

```bash
# Installer lm-evaluation-harness
pip install lm-eval

# √âvaluer le mod√®le
lm_eval --model ort \
  --model_args pretrained=models/phi-3-q4/quantized \
  --tasks arc_easy,hellaswag \
  --device cpu
```

## üêõ D√©pannage

### Erreur: Out of Memory

```bash
# Augmentez la swap ou utilisez un serveur cloud
# Exemple avec AWS EC2 t3.xlarge (16 GB RAM)
```

### Erreur: AVX512 non support√©

```bash
# Utilisez --no-avx512
python scripts/quantize-model.py --model ... --no-avx512
```

### Mod√®le trop gros pour le t√©l√©chargement

```bash
# Utilisez git-lfs pour les gros fichiers
git lfs install
git lfs track "*.onnx"
```

## üìö Ressources Compl√©mentaires

- [Documentation Optimum](https://huggingface.co/docs/optimum)
- [Guide ONNX Runtime](https://onnxruntime.ai/docs/)
- [Quantization Deep Dive](https://huggingface.co/docs/optimum/concept_guides/quantization)

## üéØ Roadmap

- [ ] Support de la quantification dynamique
- [ ] Sharding automatique des mod√®les
- [ ] Interface web pour la quantification
- [ ] Benchmarking automatis√©
- [ ] Support des mod√®les multimodaux

---

**Note**: Ce pipeline fait partie du "Chantier 3: Op√©ration Poids Plume" d'ORION.
