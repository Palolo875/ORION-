# Pipeline de Gestion des Mod√®les ORION

Ce dossier contient les outils pour g√©rer, optimiser et d√©ployer les mod√®les LLM pour ORION.

## üéØ Vue d'ensemble

Le pipeline ORION comprend trois outils principaux:

1. **Quantification** (`quantize-model.py`) - R√©duction de taille
2. **Fusion** (`merge-models.py`) - Cr√©ation de mod√®les hybrides  
3. **Sharding** (`shard-model.py`) - Chargement progressif

## üì¶ Installation des d√©pendances

```bash
# D√©pendances Python de base
pip install torch transformers safetensors

# Pour la quantification
pip install optimum[onnxruntime] onnx

# Pour la fusion de mod√®les
git clone https://github.com/cg123/mergekit.git
cd mergekit && pip install -e .

# Optionnel: PyYAML pour les configurations
pip install pyyaml
```

## üîß Outils disponibles

### 1. Quantification de mod√®les

R√©duit la taille d'un mod√®le de 50-75% avec une perte de qualit√© minimale.

**Usage:**
```bash
# Quantification standard (Q4)
python scripts/quantize-model.py \
  --model microsoft/phi-3-mini-4k-instruct \
  --output models/phi-3-q4 \
  --level q4

# Quantification agressive (Q2) pour taille minimale
python scripts/quantize-model.py \
  --model microsoft/phi-3-mini-4k-instruct \
  --output models/phi-3-q2 \
  --level q2 \
  --test
```

**Niveaux de quantification:**
- `q4` (d√©faut): 4-bit, meilleur compromis (~75% de r√©duction)
- `q3`: 3-bit, tr√®s agressif (~85% de r√©duction)
- `q2`: 2-bit, ultra-compact (~90% de r√©duction, qualit√© r√©duite)

**R√©sultat:**
```
models/phi-3-q4/
‚îú‚îÄ‚îÄ quantized/          # Mod√®le quantifi√©
‚îÇ   ‚îú‚îÄ‚îÄ model.onnx
‚îÇ   ‚îî‚îÄ‚îÄ tokenizer/
‚îú‚îÄ‚îÄ metadata.json       # M√©tadonn√©es
‚îî‚îÄ‚îÄ onnx/              # Mod√®le ONNX interm√©diaire
```

### 2. Fusion de mod√®les

Cr√©e des mod√®les hybrides en fusionnant plusieurs mod√®les.

**Usage:**
```bash
# Fusion lin√©aire (moyenne pond√©r√©e)
python scripts/merge-models.py \
  --models microsoft/phi-3-mini-4k-instruct TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
  --ratios 0.7 0.3 \
  --output models/phi-tiny-hybrid \
  --method linear

# Fusion SLERP (sph√©rique - pr√©serve mieux les propri√©t√©s)
python scripts/merge-models.py \
  --models microsoft/phi-3-mini-4k-instruct meta-llama/Llama-2-7b-chat-hf \
  --ratios 0.5 0.5 \
  --method slerp \
  --output models/phi-llama-slerp

# Fusion TIES (intelligente - √©lection des meilleurs param√®tres)
python scripts/merge-models.py \
  --models model1 model2 model3 \
  --ratios 0.5 0.3 0.2 \
  --method ties \
  --output models/ensemble
```

**M√©thodes de fusion:**
- `linear`: Moyenne pond√©r√©e simple (rapide)
- `slerp`: Interpolation sph√©rique (2 mod√®les uniquement, pr√©serve mieux)
- `ties`: Trim, Elect, and Merge (intelligent, plusieurs mod√®les)
- `dare`: Drop And REscale (avec dropout, robuste)

**Cas d'usage:**
- **Sp√©cialisation**: Fusionner un mod√®le g√©n√©ral + un mod√®le sp√©cialis√© (code, maths)
- **Multilingue**: Combiner des mod√®les de diff√©rentes langues
- **Taille optimale**: M√©langer un grand mod√®le (qualit√©) + un petit mod√®le (rapidit√©)

### 3. Sharding de mod√®les

D√©coupe un mod√®le en morceaux pour un chargement progressif.

**Usage:**
```bash
# Sharding en 4 morceaux
python scripts/shard-model.py \
  --model microsoft/phi-3-mini-4k-instruct \
  --output models/phi-3-sharded \
  --shards 4

# Sharding avec taille max par shard
python scripts/shard-model.py \
  --model microsoft/phi-3-mini-4k-instruct \
  --output models/phi-3-sharded \
  --max-shard-size 500

# Sharding d'un mod√®le local
python scripts/shard-model.py \
  --model ./models/custom_model \
  --output models/custom_sharded \
  --shards 8
```

**R√©sultat:**
```
models/phi-3-sharded/
‚îú‚îÄ‚îÄ shard_00.safetensors  # Premier shard (essentiel)
‚îú‚îÄ‚îÄ shard_01.safetensors
‚îú‚îÄ‚îÄ shard_02.safetensors
‚îú‚îÄ‚îÄ shard_03.safetensors
‚îú‚îÄ‚îÄ shard_manifest.json   # Manifeste de chargement
‚îú‚îÄ‚îÄ config.json           # Configuration du mod√®le
‚îú‚îÄ‚îÄ tokenizer.json        # Tokenizer
‚îî‚îÄ‚îÄ SHARDING_INFO.md      # Documentation
```

**Avantages:**
- ‚ö° **TTFT optimis√©**: Premier token en quelques secondes
- üì¶ **Chargement progressif**: Utilisable avant chargement complet
- üíæ **Optimisation m√©moire**: Charge seulement ce qui est n√©cessaire

## üé¨ Workflows recommand√©s

### Workflow 1: Optimisation d'un mod√®le standard

```bash
# 1. T√©l√©charger et quantifier
python scripts/quantize-model.py \
  --model microsoft/phi-3-mini-4k-instruct \
  --output models/phi-3-optimized \
  --level q4

# 2. Sharder pour chargement progressif
python scripts/shard-model.py \
  --model models/phi-3-optimized/quantized \
  --output models/phi-3-production \
  --shards 4
```

R√©sultat: Mod√®le 75% plus petit, chargement 5x plus rapide

### Workflow 2: Cr√©ation d'un mod√®le sp√©cialis√©

```bash
# 1. Fusionner un mod√®le g√©n√©ral + sp√©cialis√©
python scripts/merge-models.py \
  --models microsoft/phi-3-mini-4k-instruct google/codegemma-2b \
  --ratios 0.6 0.4 \
  --method slerp \
  --output models/phi-code-hybrid

# 2. Quantifier le r√©sultat
python scripts/quantize-model.py \
  --model models/phi-code-hybrid/merged \
  --output models/phi-code-optimized \
  --level q4

# 3. Sharder
python scripts/shard-model.py \
  --model models/phi-code-optimized/quantized \
  --output models/phi-code-production \
  --shards 4
```

R√©sultat: Mod√®le hybride optimis√© pour le code, ultra-compact

### Workflow 3: Pipeline complet pour production

```bash
# Script automatis√©
cat > optimize-for-production.sh << 'EOF'
#!/bin/bash
MODEL=$1
OUTPUT_DIR=$2

echo "üöÄ Optimisation de $MODEL pour production..."

# √âtape 1: Quantification
echo "1Ô∏è‚É£ Quantification..."
python scripts/quantize-model.py \
  --model $MODEL \
  --output ${OUTPUT_DIR}/quantized \
  --level q4

# √âtape 2: Sharding
echo "2Ô∏è‚É£ Sharding..."
python scripts/shard-model.py \
  --model ${OUTPUT_DIR}/quantized/quantized \
  --output ${OUTPUT_DIR}/production \
  --shards 4

# √âtape 3: Mise √† jour du registry
echo "3Ô∏è‚É£ Mise √† jour du Model Registry..."
# Ajouter au models.json...

echo "‚úÖ Optimisation termin√©e!"
echo "üìÅ Mod√®le disponible: ${OUTPUT_DIR}/production"
EOF

chmod +x optimize-for-production.sh

# Utilisation
./optimize-for-production.sh microsoft/phi-3-mini-4k-instruct models/phi-3
```

## üìä Comparaison des optimisations

| Technique | R√©duction taille | Impact qualit√© | Vitesse | Cas d'usage |
|-----------|------------------|----------------|---------|-------------|
| **Quantification Q4** | ~75% | Minimal (~2%) | ++ | Standard, recommand√© |
| **Quantification Q3** | ~85% | L√©ger (~5%) | +++ | Appareils limit√©s |
| **Quantification Q2** | ~90% | Moyen (~10%) | ++++ | Prototypes, tests |
| **Fusion SLERP** | Variable | Minimal | + | Sp√©cialisation |
| **Fusion TIES** | Variable | Minimal | + | Ensembles multi-t√¢ches |
| **Sharding** | 0% | Aucun | ++++ (TTFT) | Production, UX |

## üîó Int√©gration avec ORION

### Model Registry

Tous les mod√®les optimis√©s doivent √™tre r√©f√©renc√©s dans `/models.json`:

```json
{
  "models": {
    "custom-agent": {
      "id": "phi-3-optimized-q4",
      "name": "Phi-3 Optimis√©",
      "type": "causal-lm",
      "size_mb": 512,
      "urls": {
        "base": "/models/phi-3-production",
        "shards": [
          "/models/phi-3-production/shard_00.safetensors",
          "/models/phi-3-production/shard_01.safetensors",
          "/models/phi-3-production/shard_02.safetensors",
          "/models/phi-3-production/shard_03.safetensors"
        ]
      }
    }
  }
}
```

### Chargement dans ORION

```typescript
import { ProgressiveLoader } from '@/oie/utils/progressive-loader';

// Initialiser le registry
await ProgressiveLoader.initializeRegistry('/models.json');

// Charger avec Web Worker et sharding
const result = await ProgressiveLoader.loadFromRegistry(
  'custom-agent',
  {
    useWorker: true,
    shardingConfig: {
      enabled: true,
      numShards: 4,
      initialShards: 1 // TTFT optimis√©
    },
    onProgress: (progress) => {
      console.log(`Chargement: ${progress.progress * 100}%`);
    }
  }
);

console.log('Mod√®le pr√™t!', result.stats);
```

## üéØ Meilleures pratiques

### 1. Choix de la quantification

- **Production web**: Q4 (meilleur compromis)
- **Mobile/Edge**: Q3 (taille critique)
- **Prototypes**: Q2 (rapidit√© de d√©veloppement)

### 2. Fusion de mod√®les

- **2 mod√®les similaires**: SLERP
- **3+ mod√®les**: TIES ou DARE
- **Rapidit√© importante**: Linear

### 3. Sharding

- **Mod√®les < 1GB**: 2-4 shards
- **Mod√®les 1-5GB**: 4-8 shards
- **Mod√®les > 5GB**: 8-16 shards

### 4. Testing

Toujours tester le mod√®le optimis√©:

```bash
# Test apr√®s quantification
python scripts/quantize-model.py --model ... --test

# Test manuel
python << EOF
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("models/output")
tokenizer = AutoTokenizer.from_pretrained("models/output")

inputs = tokenizer("Bonjour, comment vas-tu?", return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)
print(tokenizer.decode(outputs[0]))
EOF
```

## üìö Ressources

- [mergekit Documentation](https://github.com/cg123/mergekit)
- [ONNX Runtime Quantization](https://onnxruntime.ai/docs/performance/quantization.html)
- [SafeTensors Format](https://huggingface.co/docs/safetensors)
- [WebLLM Documentation](https://webllm.mlc.ai/)

## üêõ D√©pannage

### Erreur: "Out of memory"

```bash
# R√©duire la taille du batch ou utiliser low_cpu_mem_usage
python scripts/quantize-model.py --model ... --low-cpu-mem
```

### Erreur: "Model architecture not compatible"

```bash
# Utiliser --allow-crimes avec mergekit (exp√©rimental)
# Voir merge-models.py ligne 89
```

### Mod√®le quantifi√© donne des r√©sultats √©tranges

```bash
# Essayer Q4 au lieu de Q2/Q3
# V√©rifier que le mod√®le source est compatible avec ONNX
```

## üìù Notes

- Les scripts cr√©ent automatiquement des fichiers `metadata.json` avec toutes les infos
- Les shards sont au format `.safetensors` (plus s√ªr que `.bin`)
- Le Model Registry est valid√© par `models.schema.json`
- Tous les scripts supportent `--help` pour plus d'options

## üöÄ Prochaines √©tapes

1. Ajouter support pour d'autres formats (GGUF, GPTQ)
2. Automatiser le benchmark des mod√®les optimis√©s
3. Int√©grer avec CI/CD pour optimisation automatique
4. Dashboard web pour g√©rer le pipeline
