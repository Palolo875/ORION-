#!/usr/bin/env python3
"""
Script de quantification de mod√®les pour ORION
Permet de r√©duire la taille des mod√®les tout en maintenant la qualit√©

Usage:
    python scripts/quantize-model.py --model microsoft/phi-3-mini-4k-instruct --output models/phi-3-q4
    
    Options:
    --model: ID du mod√®le Hugging Face (requis)
    --output: Chemin de sortie (requis)
    --level: Niveau de quantification (q4, q3, q2) - d√©faut: q4
    --avx512: Utiliser AVX512 pour de meilleures performances (d√©faut: True)
    --test: Tester le mod√®le quantifi√© apr√®s g√©n√©ration
"""

import argparse
import os
import sys
from pathlib import Path

def check_dependencies():
    """V√©rifie que toutes les d√©pendances sont install√©es"""
    required_packages = {
        'optimum': 'optimum[onnxruntime]',
        'onnx': 'onnx',
        'transformers': 'transformers'
    }
    
    missing = []
    for package, install_name in required_packages.items():
        try:
            __import__(package)
        except ImportError:
            missing.append(install_name)
    
    if missing:
        print("‚ùå D√©pendances manquantes. Installez-les avec:")
        print(f"   pip install {' '.join(missing)}")
        sys.exit(1)

def quantize_model(model_name: str, output_path: str, level: str = 'q4', use_avx512: bool = True):
    """
    Quantifie un mod√®le Hugging Face
    
    Args:
        model_name: ID du mod√®le sur Hugging Face
        output_path: Chemin o√π sauvegarder le mod√®le quantifi√©
        level: Niveau de quantification (q4, q3, q2)
        use_avx512: Utiliser AVX512 pour l'optimisation
    """
    from optimum.onnxruntime import ORTQuantizer, ORTModelForCausalLM
    from optimum.onnxruntime.configuration import AutoQuantizationConfig
    from transformers import AutoTokenizer
    
    print(f"üöÄ D√©marrage de la quantification de {model_name}")
    print(f"üìä Niveau: {level}")
    print(f"üíæ Sortie: {output_path}")
    print()
    
    # Cr√©er le dossier de sortie
    output_dir = Path(output_path)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # √âtape 1: Charger le mod√®le et le convertir en ONNX
    print("1Ô∏è‚É£ Chargement et conversion du mod√®le en ONNX...")
    try:
        onnx_path = output_dir / "onnx"
        model = ORTModelForCausalLM.from_pretrained(
            model_name, 
            export=True,
            provider="CPUExecutionProvider"
        )
        model.save_pretrained(onnx_path)
        print(f"   ‚úÖ Mod√®le ONNX sauvegard√© dans {onnx_path}")
    except Exception as e:
        print(f"   ‚ùå Erreur lors de la conversion ONNX: {e}")
        raise
    
    # √âtape 2: Configurer la quantification
    print(f"\n2Ô∏è‚É£ Configuration de la quantification {level}...")
    
    quantization_configs = {
        'q4': {
            'is_static': False,
            'per_channel': False,
            'format': 'QOperator',
            'weight_type': 'QInt8'
        },
        'q3': {
            'is_static': False,
            'per_channel': True,
            'format': 'QDQ',
            'weight_type': 'QUInt8'
        },
        'q2': {
            'is_static': False,
            'per_channel': True,
            'format': 'QDQ',
            'weight_type': 'QUInt8',
            'reduce_range': True
        }
    }
    
    if level not in quantization_configs:
        print(f"   ‚ö†Ô∏è  Niveau '{level}' inconnu, utilisation de q4 par d√©faut")
        level = 'q4'
    
    config_params = quantization_configs[level]
    
    if use_avx512:
        qconfig = AutoQuantizationConfig.avx512_vnni(**config_params)
    else:
        qconfig = AutoQuantizationConfig.arm64(**config_params)
    
    print(f"   ‚úÖ Configuration: {config_params}")
    
    # √âtape 3: Quantifier le mod√®le
    print(f"\n3Ô∏è‚É£ Quantification du mod√®le...")
    try:
        quantized_path = output_dir / "quantized"
        quantizer = ORTQuantizer.from_pretrained(model)
        quantizer.quantize(
            save_dir=quantized_path,
            quantization_config=qconfig
        )
        print(f"   ‚úÖ Mod√®le quantifi√© sauvegard√© dans {quantized_path}")
    except Exception as e:
        print(f"   ‚ùå Erreur lors de la quantification: {e}")
        raise
    
    # √âtape 4: Sauvegarder le tokenizer
    print(f"\n4Ô∏è‚É£ Sauvegarde du tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.save_pretrained(quantized_path)
        print(f"   ‚úÖ Tokenizer sauvegard√©")
    except Exception as e:
        print(f"   ‚ùå Erreur lors de la sauvegarde du tokenizer: {e}")
        raise
    
    # √âtape 5: Calculer les statistiques de taille
    print(f"\n5Ô∏è‚É£ Statistiques de compression...")
    
    def get_size(path: Path) -> int:
        """Calcule la taille totale d'un dossier"""
        return sum(f.stat().st_size for f in path.rglob('*') if f.is_file())
    
    onnx_size = get_size(onnx_path) / (1024 * 1024)  # MB
    quantized_size = get_size(quantized_path) / (1024 * 1024)  # MB
    compression_ratio = (1 - quantized_size / onnx_size) * 100
    
    print(f"   üìä Taille ONNX: {onnx_size:.1f} MB")
    print(f"   üìä Taille quantifi√©e: {quantized_size:.1f} MB")
    print(f"   üìä Compression: {compression_ratio:.1f}%")
    
    print(f"\n‚úÖ Quantification termin√©e avec succ√®s!")
    print(f"üìÅ Mod√®le disponible dans: {quantized_path}")
    
    return quantized_path

def test_quantized_model(model_path: str):
    """Teste le mod√®le quantifi√© avec une simple g√©n√©ration"""
    from optimum.onnxruntime import ORTModelForCausalLM
    from transformers import AutoTokenizer
    
    print(f"\nüß™ Test du mod√®le quantifi√©...")
    
    try:
        # Charger le mod√®le et le tokenizer
        model = ORTModelForCausalLM.from_pretrained(model_path)
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # Test de g√©n√©ration simple
        prompt = "Hello, my name is"
        inputs = tokenizer(prompt, return_tensors="pt")
        
        print(f"   Prompt: '{prompt}'")
        
        outputs = model.generate(**inputs, max_new_tokens=20)
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print(f"   G√©n√©ration: '{generated_text}'")
        print(f"   ‚úÖ Test r√©ussi!")
        
    except Exception as e:
        print(f"   ‚ùå Erreur lors du test: {e}")
        raise

def create_metadata(output_path: str, model_name: str, level: str):
    """Cr√©e un fichier metadata.json pour le mod√®le quantifi√©"""
    import json
    from datetime import datetime
    
    metadata = {
        "original_model": model_name,
        "quantization_level": level,
        "created_at": datetime.now().isoformat(),
        "tool": "ORION Quantization Pipeline",
        "usage": {
            "transformers": f"model = ORTModelForCausalLM.from_pretrained('{output_path}')",
            "web": "H√©bergez ce dossier et r√©f√©rencez-le dans vos agents ORION"
        }
    }
    
    metadata_path = Path(output_path) / "metadata.json"
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"\nüìÑ M√©tadonn√©es sauvegard√©es dans {metadata_path}")

def main():
    parser = argparse.ArgumentParser(
        description="Pipeline de quantification de mod√®les pour ORION",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  # Quantification standard (q4)
  python scripts/quantize-model.py --model microsoft/phi-3-mini-4k-instruct --output models/phi-3-q4
  
  # Quantification agressive (q2) pour une taille minimale
  python scripts/quantize-model.py --model microsoft/phi-3-mini-4k-instruct --output models/phi-3-q2 --level q2
  
  # Avec test du mod√®le apr√®s quantification
  python scripts/quantize-model.py --model microsoft/phi-3-mini-4k-instruct --output models/phi-3-q4 --test
        """
    )
    
    parser.add_argument(
        '--model',
        type=str,
        required=True,
        help='ID du mod√®le Hugging Face (ex: microsoft/phi-3-mini-4k-instruct)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        required=True,
        help='Chemin de sortie pour le mod√®le quantifi√©'
    )
    
    parser.add_argument(
        '--level',
        type=str,
        default='q4',
        choices=['q4', 'q3', 'q2'],
        help='Niveau de quantification (d√©faut: q4)'
    )
    
    parser.add_argument(
        '--avx512',
        action='store_true',
        default=True,
        help='Utiliser AVX512 pour de meilleures performances (d√©faut: True)'
    )
    
    parser.add_argument(
        '--test',
        action='store_true',
        help='Tester le mod√®le apr√®s quantification'
    )
    
    args = parser.parse_args()
    
    # V√©rifier les d√©pendances
    check_dependencies()
    
    # Quantifier le mod√®le
    quantized_path = quantize_model(
        model_name=args.model,
        output_path=args.output,
        level=args.level,
        use_avx512=args.avx512
    )
    
    # Cr√©er les m√©tadonn√©es
    create_metadata(args.output, args.model, args.level)
    
    # Tester si demand√©
    if args.test:
        test_quantized_model(quantized_path)

if __name__ == '__main__':
    main()
