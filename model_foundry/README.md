# üè≠ ORION Model Foundry

**Fonderie de Mod√®les AI pour ORION** - Cr√©ez, fusionnez et optimisez vos propres mod√®les d'IA

## üìã Vue d'ensemble

La Model Foundry est un environnement d√©di√© pour cr√©er des agents AI hybrides en fusionnant des mod√®les existants. Elle permet de combiner les forces de plusieurs mod√®les pour cr√©er des agents uniques et optimis√©s pour des t√¢ches sp√©cifiques.

### Pourquoi fusionner des mod√®les ?

- **Combiner des expertises** : Cr√©ez un agent qui excelle en code ET en multilingue
- **R√©duire la complexit√©** : Un seul mod√®le au lieu de deux agents s√©par√©s
- **Optimiser les ressources** : Moins de RAM, moins de temps de chargement
- **Personnaliser** : Ajustez le ratio pour privil√©gier certaines capacit√©s

## üèóÔ∏è Architecture

```
model_foundry/
‚îú‚îÄ‚îÄ recipes/              # Recettes de fusion (YAML)
‚îÇ   ‚îî‚îÄ‚îÄ dev-polyglot-v1.yml
‚îú‚îÄ‚îÄ merged_models/        # Mod√®les fusionn√©s (sortie brute)
‚îú‚îÄ‚îÄ optimized_models/     # Mod√®les optimis√©s pour le web
‚îú‚îÄ‚îÄ merge_models.py       # Script de fusion
‚îú‚îÄ‚îÄ optimize_for_web.py   # Script d'optimisation
‚îú‚îÄ‚îÄ foundry.sh           # Orchestrateur principal
‚îú‚îÄ‚îÄ pyproject.toml       # Configuration Poetry
‚îî‚îÄ‚îÄ README.md
```

## üöÄ D√©marrage rapide

### 1. Installation

**Pr√©requis :**
- Python 3.10+
- Poetry ([Installation](https://python-poetry.org/docs/#installation))
- 16 GB RAM minimum (recommand√©: 32 GB)
- GPU optionnel (acc√©l√®re le processus)

**Initialiser l'environnement :**

```bash
cd model_foundry
./foundry.sh init
```

Ceci installera toutes les d√©pendances n√©cessaires dans un environnement virtuel isol√©.

### 2. Voir les recettes disponibles

```bash
./foundry.sh list
```

### 3. Cr√©er votre premier agent hybride

**Option A : Pipeline complet (recommand√©)**

```bash
./foundry.sh pipeline recipes/dev-polyglot-v1.yml
```

Cette commande effectue automatiquement :
1. La fusion des mod√®les
2. L'optimisation pour le web
3. La cr√©ation des m√©tadonn√©es

**Option B : √âtape par √©tape**

```bash
# 1. Fusionner les mod√®les
./foundry.sh merge recipes/dev-polyglot-v1.yml

# 2. Optimiser pour le web
./foundry.sh optimize merged_models/ORION-dev-polyglot-v1
```

## üìù Cr√©er une recette personnalis√©e

Une recette est un fichier YAML qui d√©crit comment fusionner des mod√®les.

### Structure d'une recette

```yaml
# recipes/my-custom-agent-v1.yml

models:
  - model: google/codegemma-2b        # Mod√®le 1
  - model: Qwen/Qwen2-1.5B-Instruct   # Mod√®le 2

merge_method: slerp  # M√©thode de fusion

parameters:
  t: 0.4  # Ratio: 0.0 = 100% mod√®le 1, 1.0 = 100% mod√®le 2

dtype: bfloat16  # Pr√©cision (bfloat16 recommand√©)

metadata:
  description: "Mon agent personnalis√©"
  capabilities: ["code", "chat"]
```

### Param√®tres de fusion

- **t = 0.0** : 100% du premier mod√®le
- **t = 0.3** : 70% premier mod√®le + 30% second
- **t = 0.5** : √âquilibre parfait (50/50)
- **t = 0.7** : 30% premier mod√®le + 70% second
- **t = 1.0** : 100% du second mod√®le

**Exemple :** Pour un agent qui privil√©gie le code mais avec un support multilingue :
- `t: 0.3` ‚Üí 70% CodeGemma + 30% Qwen2

### Exemples de recettes

**Agent Documentation (Expert en r√©daction technique) :**

```yaml
models:
  - model: google/gemma-2b-it
  - model: mistralai/Mistral-7B-Instruct-v0.2

merge_method: slerp
parameters:
  t: 0.6  # Favorise Mistral pour la cr√©ativit√©

metadata:
  description: "Expert en documentation technique"
```

**Agent Analyse de Donn√©es (Code + Maths) :**

```yaml
models:
  - model: google/codegemma-2b
  - model: meta-llama/Llama-3.2-3B-Instruct

merge_method: slerp
parameters:
  t: 0.5  # √âquilibre code/raisonnement

metadata:
  description: "Analyse de donn√©es et visualisation"
```

## üîß Utilisation avanc√©e

### Scripts Python directement

**Fusion avec options personnalis√©es :**

```bash
poetry run python merge_models.py \
  --recipe recipes/dev-polyglot-v1.yml \
  --output merged_models/my-custom-name
```

**Optimisation avec quantification :**

```bash
poetry run python optimize_for_web.py \
  --model merged_models/ORION-dev-polyglot-v1 \
  --output optimized_models/custom-output \
  --quantization q4f16_1 \
  --name "ORION-Custom-Agent-v1"
```

### Types de quantification

- `q4f16_1` : Quantification 4-bit (recommand√©, ~70% de r√©duction)
- `q8` : Quantification 8-bit (~50% de r√©duction)
- `fp16` : Float 16-bit (~50% de r√©duction)
- `fp32` : Float 32-bit (aucune compression)

## üîó Int√©gration dans ORION

Une fois votre mod√®le cr√©√© et optimis√© :

### 1. Ajouter au model registry

√âditez `/workspace/models.json` :

```json
{
  "models": {
    "hybrid-developer": {
      "id": "ORION-Dev-Polyglot-v1-q4f16_1-MLC",
      "name": "ORION Dev Polyglot",
      "type": "causal-lm",
      "size_mb": 1200,
      "quality": "high",
      "speed": "fast",
      "description": "Agent hybride code + multilingue",
      "capabilities": ["code", "multilingual", "chat", "reasoning"],
      "min_ram_gb": 4,
      "prompt_format": {
        "system_prefix": "<|system|>\n",
        "user_prefix": "<|user|>\n",
        "assistant_prefix": "<|assistant|>\n",
        "eos_token": "<|end|>"
      },
      "urls": {
        "base": "/models/ORION-Dev-Polyglot-v1-q4f16_1/",
        "shards": null
      },
      "config": {
        "max_tokens": 4096,
        "temperature": 0.5,
        "top_p": 0.9
      }
    }
  }
}
```

### 2. Cr√©er l'agent correspondant

Cr√©ez `/workspace/src/oie/agents/hybrid-developer.ts` :

```typescript
/**
 * Agent Hybrid Developer - Expert en code + multilingue
 * Mod√®le fusionn√©: CodeGemma 2B + Qwen2 1.5B
 */

import { BaseAgent } from './base-agent';
import { AgentInput, AgentOutput } from '../types/agent.types';
import { OPTIMIZATION_PRESETS } from '../types/optimization.types';
import { ProgressiveLoader } from '../utils/progressive-loader';

export class HybridDeveloperAgent extends BaseAgent {
  private engine: any = null;
  
  constructor() {
    super({
      id: 'hybrid-developer',
      name: 'Hybrid Developer',
      capabilities: ['code_generation', 'multilingual', 'chat'],
      modelSize: 1200,
      priority: 9,
      modelId: 'ORION-Dev-Polyglot-v1-q4f16_1-MLC'
    });
  }
  
  protected async loadModel(): Promise<void> {
    console.log(`[HybridDeveloper] Chargement du mod√®le hybride...`);
    
    const result = await ProgressiveLoader.loadModel(
      this.metadata.modelId,
      { enabled: true, numShards: 4, initialShards: 1 },
      (progress) => {
        console.log(`[HybridDeveloper] ${(progress.progress * 100).toFixed(1)}%`);
      }
    );
    
    this.engine = result.engine;
  }
  
  protected async unloadModel(): Promise<void> {
    if (this.engine) {
      this.engine = null;
    }
  }
  
  protected async processInternal(input: AgentInput): Promise<AgentOutput> {
    const messages = [
      {
        role: 'system',
        content: `Tu es un d√©veloppeur expert polyglotte. Tu maitrises:
- Programmation: Python, JavaScript, TypeScript, et plus
- Langues: Fran√ßais, Anglais, Espagnol, Chinois, et plus
Fournis des r√©ponses pr√©cises et bien structur√©es.`
      },
      {
        role: 'user',
        content: input.content
      }
    ];
    
    const response = await this.engine.chat.completions.create({
      messages,
      temperature: input.temperature || 0.5,
      max_tokens: input.maxTokens || 2000
    });
    
    return {
      agentId: this.metadata.id,
      content: response.choices[0].message.content,
      confidence: 85,
      processingTime: 0
    };
  }
}
```

### 3. Enregistrer l'agent

√âditez `/workspace/src/oie/agents/index.ts` :

```typescript
export { HybridDeveloperAgent } from './hybrid-developer';
```

## üìä M√©triques et statistiques

### Tailles typiques

| Mod√®le | Original | Fusionn√© | Optimis√© (q4) | Gain |
|--------|----------|----------|---------------|------|
| CodeGemma 2B | 2.5 GB | - | - | - |
| Qwen2 1.5B | 1.8 GB | - | - | - |
| **Dev Polyglot** | - | 4.3 GB | **1.2 GB** | **72%** |

### Performance

- **Temps de fusion** : 10-30 minutes (selon CPU/GPU)
- **Temps d'optimisation** : 5-15 minutes
- **Chargement dans le navigateur** : ~3-5 secondes (avec sharding)

## üî¨ Cas d'usage avanc√©s

### 1. Agent sp√©cialis√© domaine

Fusionnez un mod√®le g√©n√©ral avec un mod√®le sp√©cialis√© (m√©dical, juridique, etc.)

### 2. Am√©lioration progressive

Cr√©ez des versions it√©ratives (v1, v2, v3) en ajustant le ratio

### 3. Multi-fusion

Fusionnez plus de 2 mod√®les en cha√Æne :
```bash
# Fusion A + B = AB
./foundry.sh merge recipes/step1-ab.yml

# Fusion AB + C = ABC
./foundry.sh merge recipes/step2-abc.yml
```

## ‚ö†Ô∏è Limitations et consid√©rations

1. **Compatibilit√© architecturale** : Les mod√®les doivent avoir des architectures compatibles
2. **Qualit√© variable** : La fusion ne garantit pas toujours de meilleurs r√©sultats
3. **Taille** : Le mod√®le fusionn√© peut √™tre plus gros que les parents
4. **Licence** : Respectez les licences des mod√®les sources
5. **Ressources** : La fusion n√©cessite beaucoup de RAM (2-3x la taille des mod√®les)

## üêõ D√©pannage

### Erreur : "Out of memory"

- **Solution** : Utilisez un serveur avec plus de RAM ou activez le swap
- Fermez les applications gourmandes en m√©moire
- Utilisez `dtype: fp16` au lieu de `fp32`

### Erreur : "Model architectures incompatible"

- **Solution** : V√©rifiez que les deux mod√®les ont des architectures similaires
- Privil√©giez les mod√®les de la m√™me famille (ex: Gemma + CodeGemma)

### Les mod√®les ne se t√©l√©chargent pas

- **Solution** : V√©rifiez votre connexion internet
- Configurez votre token Hugging Face : `huggingface-cli login`
- Certains mod√®les n√©cessitent une acceptation de licence

## üìö Ressources

- [Hugging Face Model Hub](https://huggingface.co/models)
- [Transformers Documentation](https://huggingface.co/docs/transformers)
- [Model Merging Guide](https://huggingface.co/blog/mlabonne/merge-models)
- [ORION Documentation](../README.md)

## ü§ù Contribution

Pour sugg√©rer des am√©liorations ou partager vos recettes :

1. Cr√©ez une nouvelle recette dans `recipes/`
2. Documentez les r√©sultats obtenus
3. Partagez vos trouvailles avec l'√©quipe

## üìù Changelog

### Version 1.0.0 (2025-10-22)

- ‚ú® Syst√®me de fusion par moyenne pond√©r√©e (SLERP)
- ‚ú® Optimisation automatique pour le web
- ‚ú® Support de la quantification multi-niveaux
- ‚ú® Sharding automatique pour chargement progressif
- ‚ú® Interface CLI avec `foundry.sh`
- üìù Recette exemple : Dev Polyglot v1
- üìö Documentation compl√®te

## üìÑ Licence

Ce projet fait partie d'ORION et suit la m√™me licence.

---

**Cr√©√© avec ‚ù§Ô∏è par l'√©quipe ORION** üöÄ
