# ORION Model Foundry ğŸ”¨

> Pipeline de crÃ©ation, fusion et optimisation de modÃ¨les pour l'Ã©cosystÃ¨me ORION

## ğŸ¯ Vue d'ensemble

La **Model Foundry** (Fonderie de ModÃ¨les) est l'atelier oÃ¹ sont crÃ©Ã©s, fusionnÃ©s et optimisÃ©s les modÃ¨les IA qui alimentent l'Orion Inference Engine (OIE). C'est un pipeline Python sÃ©parÃ© qui transforme des modÃ¨les bruts en variants optimisÃ©s pour le web.

## ğŸ—ï¸ Architecture

```
model_foundry/
â”œâ”€â”€ recipes/                  # Recettes de fusion au format YAML
â”‚   â”œâ”€â”€ dev-polyglot-v1.yml  # Agent DÃ©veloppeur Polyglotte
â”‚   â””â”€â”€ ...
â”œâ”€â”€ merged_models/            # ModÃ¨les fusionnÃ©s (sortie)
â”œâ”€â”€ optimized_models/         # ModÃ¨les optimisÃ©s pour le web
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ merge_models.py      # Fusion de modÃ¨les avec mergekit
â”‚   â”œâ”€â”€ quantize_model.py    # Quantification ONNX avec optimum
â”‚   â”œâ”€â”€ shard_model.py       # DÃ©coupage en shards
â”‚   â””â”€â”€ optimize_pipeline.py # Pipeline complet
â”œâ”€â”€ pyproject.toml           # Configuration Poetry
â”œâ”€â”€ requirements.txt         # DÃ©pendances Python
â””â”€â”€ Makefile                 # Commandes automatisÃ©es
```

## ğŸ“¦ Installation

### PrÃ©requis

- Python 3.10+
- Poetry (gestionnaire de dÃ©pendances)
- CUDA (optionnel, pour accÃ©lÃ©ration GPU)

### Installation rapide

```bash
cd model_foundry

# Option 1: Avec Poetry (recommandÃ©)
poetry install

# Option 2: Avec pip
pip install -r requirements.txt
```

## ğŸš€ Quick Start

### 1. CrÃ©er un modÃ¨le hybride

```bash
# Activer l'environnement Poetry
poetry shell

# Fusionner deux modÃ¨les
mergekit-yaml recipes/dev-polyglot-v1.yml merged_models/ORION-Dev-Polyglot-v1
```

### 2. Optimiser pour le web

```bash
# Quantifier + Sharder en une commande
python optimize_pipeline.py \
  --model_path merged_models/ORION-Dev-Polyglot-v1 \
  --output_path ../public/models/ORION-Dev-Polyglot-v1-q4 \
  --quantization q4 \
  --shard_size 100
```

### 3. Utiliser dans l'OIE

Le modÃ¨le optimisÃ© est automatiquement ajoutÃ© Ã  `models.json` et prÃªt Ã  Ãªtre utilisÃ© !

## ğŸ§ª Recettes de Fusion

### Format d'une recette

Les recettes sont des fichiers YAML qui dÃ©crivent comment fusionner deux modÃ¨les ou plus.

```yaml
# recipes/dev-polyglot-v1.yml
models:
  - model: google/codegemma-2b-it
  - model: Qwen/Qwen2-1.5B-Instruct

merge_method: slerp  # Spherical Linear Interpolation

parameters:
  t: 0.4  # 60% premier modÃ¨le, 40% second modÃ¨le

dtype: bfloat16
```

### MÃ©thodes de fusion disponibles

- **SLERP** (Spherical Linear Interpolation): Interpole les poids sur une sphÃ¨re
- **Linear**: Moyenne pondÃ©rÃ©e simple
- **Task Arithmetic**: Fusion basÃ©e sur des tÃ¢ches spÃ©cifiques
- **TIES**: RÃ©solution des conflits de fusion
- **DARE**: Drop And REscale

## âš™ï¸ StratÃ©gies d'optimisation

### Quantification

RÃ©duit la prÃ©cision des poids pour diminuer la taille du modÃ¨le.

| Niveau | PrÃ©cision | Taille | QualitÃ© | Cas d'usage |
|--------|-----------|--------|---------|-------------|
| **FP16** | 16-bit float | 100% | Excellente | RÃ©fÃ©rence |
| **q4** | 4-bit int | ~25% | TrÃ¨s bonne | DÃ©faut recommandÃ© |
| **q3** | 3-bit int | ~19% | Bonne | ModÃ¨les robustes |
| **q2** | 2-bit int | ~12% | Correcte | Ultra-compact |

**Commande:**
```bash
python quantize_model.py \
  --model microsoft/Phi-3-mini-4k-instruct \
  --output optimized_models/Phi-3-mini-q3 \
  --quantization q3
```

### Sharding

DÃ©coupe un modÃ¨le en plusieurs fichiers pour chargement progressif.

**Avantages:**
- Chargement partiel du modÃ¨le
- PremiÃ¨re infÃ©rence plus rapide
- Hydratation progressive en arriÃ¨re-plan

**Commande:**
```bash
python shard_model.py \
  --model_path merged_models/ORION-Dev-Polyglot-v1 \
  --output_path optimized_models/ORION-Dev-Polyglot-v1-sharded \
  --shard_size 100  # Mo par shard
```

## ğŸ“Š Validation de qualitÃ©

AprÃ¨s optimisation, validez que le modÃ¨le fonctionne correctement:

```bash
# Tests automatisÃ©s
python scripts/validate_model.py \
  --model_path optimized_models/Phi-3-mini-q3 \
  --benchmark hellaswag,arc

# Test manuel
python scripts/test_inference.py \
  --model optimized_models/Phi-3-mini-q3 \
  --prompt "Ã‰cris une fonction Python qui..."
```

## ğŸ¨ ModÃ¨les hybrides ORION

### ORION-Dev-Polyglot v1

**Fusion:** CodeGemma 2B + Qwen2 1.5B (ratio 60/40)
**Objectif:** Expert en code multilingue
**Avantages:** 
- Remplace 2 agents (CodeAgent + MultilingualAgent)
- Ã‰conomise ~700 Mo de RAM
- Performance combinÃ©e supÃ©rieure

**CrÃ©ation:**
```bash
mergekit-yaml recipes/dev-polyglot-v1.yml merged_models/ORION-Dev-Polyglot-v1
python optimize_pipeline.py \
  --model_path merged_models/ORION-Dev-Polyglot-v1 \
  --output_path ../public/models/ORION-Dev-Polyglot-v1-q4 \
  --quantization q4 \
  --shard_size 100
```

## ğŸ”§ Makefile - Commandes rapides

```bash
# Installer les dÃ©pendances
make install

# CrÃ©er tous les modÃ¨les hybrides
make build-all

# CrÃ©er un modÃ¨le spÃ©cifique
make build-dev-polyglot

# Nettoyer les fichiers temporaires
make clean

# Tests de validation
make test

# Afficher l'aide
make help
```

## ğŸ“ Workflow de dÃ©veloppement

### 1. Concevoir une recette

CrÃ©ez une nouvelle recette dans `recipes/my-model-v1.yml`:

```yaml
models:
  - model: base/model-1
  - model: base/model-2

merge_method: slerp
parameters:
  t: 0.5

dtype: bfloat16
```

### 2. Tester la fusion

```bash
mergekit-yaml recipes/my-model-v1.yml merged_models/my-model-v1 --copy-tokenizer
```

### 3. Valider la qualitÃ©

```bash
python scripts/validate_model.py --model_path merged_models/my-model-v1
```

### 4. Optimiser pour production

```bash
python optimize_pipeline.py \
  --model_path merged_models/my-model-v1 \
  --output_path ../public/models/my-model-v1-q4 \
  --quantization q4 \
  --shard_size 100
```

### 5. IntÃ©grer dans ORION

Ajoutez l'entrÃ©e dans `models.json`:

```json
{
  "my-custom-agent": {
    "id": "my-model-v1-q4",
    "name": "My Custom Model",
    "size_mb": 1200,
    "urls": {
      "base": "/models/my-model-v1-q4/"
    },
    "metadata": {
      "fusion": {
        "method": "slerp",
        "sources": ["base/model-1", "base/model-2"],
        "created_by": "ORION Model Foundry"
      }
    }
  }
}
```

## ğŸ›¡ï¸ Bonnes pratiques

1. **Versionnez vos recettes** - Utilisez git pour tracker les changements
2. **Testez avant de fusionner** - Validez chaque modÃ¨le parent individuellement
3. **Documentez les ratios** - Notez pourquoi vous avez choisi un ratio spÃ©cifique
4. **Benchmarkez** - Mesurez la performance avant/aprÃ¨s fusion
5. **Gardez les mÃ©tadonnÃ©es** - Tracez l'origine de chaque modÃ¨le fusionnÃ©

## ğŸ”¬ ExpÃ©rimentation

### Tester diffÃ©rents niveaux de quantification

```bash
# CrÃ©er plusieurs variants
for quant in q2 q3 q4; do
  python quantize_model.py \
    --model my-model \
    --output my-model-$quant \
    --quantization $quant
done

# Comparer les performances
python scripts/benchmark_variants.py my-model-*
```

### Trouver le ratio optimal

```bash
# Tester plusieurs ratios
for ratio in 0.3 0.4 0.5 0.6 0.7; do
  echo "Testing ratio $ratio"
  # Modifier la recette
  sed "s/t: .*/t: $ratio/" recipes/base.yml > recipes/test-$ratio.yml
  # Fusionner
  mergekit-yaml recipes/test-$ratio.yml merged_models/test-$ratio
  # Valider
  python scripts/validate_model.py --model_path merged_models/test-$ratio
done
```

## ğŸ“š Ressources

- [Mergekit Documentation](https://github.com/cg123/mergekit)
- [Optimum ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/overview)
- [ONNX Model Zoo](https://github.com/onnx/models)
- [Model Compression Techniques](https://arxiv.org/abs/2308.07633)

## ğŸ¤ Contribution

Pour ajouter une nouvelle recette ou amÃ©liorer le pipeline:

1. CrÃ©ez une branche: `git checkout -b feature/new-model`
2. Ajoutez votre recette dans `recipes/`
3. Testez: `make test`
4. Documentez dans ce README
5. Soumettez une PR

## ğŸ“„ Licence

Ce pipeline fait partie du projet ORION. Voir LICENSE Ã  la racine du projet.

---

**Made with â¤ï¸ by the ORION Team**
